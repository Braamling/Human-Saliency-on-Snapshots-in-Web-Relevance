{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from preprocessing.contextualFeaturesGenerator.utils.LETORIterator import LETORIterator\n",
    "from scipy.stats import ttest_rel\n",
    "from utils.evaluate import Evaluate \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_all_LTR_metrics(file, ids, metrics):\n",
    "    results = []\n",
    "    for idx in ids:\n",
    "        df = pd.read_pickle(file.format(idx))\n",
    "        for column in df:\n",
    "            if type(column) is int:\n",
    "                # Get a ranking with the respective scores.\n",
    "                ranking = df[str(column) + \"_s\"].as_matrix()\n",
    "                \n",
    "                # Remove all nans\n",
    "                ranking = [int(x) for x in ranking if not math.isnan(float(x))]\n",
    "                \n",
    "                # Calculate the evaluation scores\n",
    "                scores = Evaluate.compute_scores(ranking)\n",
    "                \n",
    "                # Append all evaluation scores together with its query id in the correct order\n",
    "                results.append([column] + [scores[i] for i in metrics])\n",
    "                \n",
    "    df = pd.DataFrame(results, columns=[\"query_id\"] + metrics)\n",
    "    return df.groupby(\"query_id\", as_index=False).mean().sort_values(by=[\"query_id\"])\n",
    "\n",
    "def get_baseline_df(test_file, score_file, metrics):\n",
    "    queries = []\n",
    "    scores = []\n",
    "    prev_query_id = -1\n",
    "    for i in range(1, 6):\n",
    "        iterator = LETORIterator(test_file.format(i))\n",
    "        with open(score_file.format(i), \"r\") as f:\n",
    "            for line, (d_query_id, doc_id, rel_score, _) in zip(f, iterator.feature_iterator()):\n",
    "                s_query_id, _, score = line.rstrip().split(\"\\t\")\n",
    "                s_query_id, score = int(s_query_id), float(score)\n",
    "\n",
    "                assert int(s_query_id) == int(d_query_id), str(s_query_id) + \" != \" + str(s_query_id)\n",
    "                \n",
    "                if prev_query_id == -1:\n",
    "                    prev_query_id = s_query_id\n",
    "                    \n",
    "                \n",
    "                if s_query_id != prev_query_id:\n",
    "                    prev_query_id = s_query_id\n",
    "                    if len(scores) > 0:\n",
    "                        scores = sorted(scores, key=lambda x: -x[1])\n",
    "                        queries.append((s_query_id, scores))\n",
    "                        scores = []\n",
    "                \n",
    "\n",
    "\n",
    "                scores.append((int(rel_score), score))\n",
    "    if len(scores) > 0:\n",
    "        scores = sorted(scores, key=lambda x: -x[1])\n",
    "        queries.append((s_query_id, scores))\n",
    "        scores = []\n",
    "    \n",
    "    results = []\n",
    "    for query, rank in queries:\n",
    "        ranking = list(zip(*rank))[0]\n",
    "\n",
    "        # Calculate the evaluation scores\n",
    "        scores = Evaluate.compute_scores(ranking)\n",
    "                \n",
    "        # Append all evaluation scores together with its query id in the correct order\n",
    "        results.append([query] + [scores[i] for i in metrics])\n",
    "   \n",
    "    df = pd.DataFrame(results, columns=[\"query_id\"] + metrics)\n",
    "    return df.sort_values(by=[\"query_id\"])\n",
    "\n",
    "def t_test(df_1, df_2, metrics):\n",
    "    results = []\n",
    "    for metric in metrics:\n",
    "        results.append((metric, ttest_rel(df_1[metric], df_2[metric]).pvalue))\n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle_path = \"storage/logs\"\n",
    "files = [\"_baseline_masks_{}.pkl\",\n",
    "        \"_ViP_snapshots_{}.pkl\",\n",
    "        \"_ViP_highlights_{}.pkl\",\n",
    "        \"_vgg16_snapshots_{}.pkl\",\n",
    "        \"_vgg16_highlights_{}.pkl\",\n",
    "        \"_vgg16_saliency_{}.pkl\"]\n",
    "metrics = [\"p@1\",\"p@5\",\"p@10\",\"ndcg@1\",\"ndcg@5\",\"ndcg@10\",\"map\"]\n",
    "\n",
    "ids = range(1, 26)\n",
    "vis_dfs = []\n",
    "for file in files: \n",
    "    vis_dfs.append(get_all_LTR_metrics(os.path.join(pickle_path, file), ids, metrics))\n",
    "    \n",
    "rankboost_df = get_baseline_df(\"storage/clueweb12_3.0/Fold{}/vali.txt\", \"storage/baseline/scores/rankboost_{}\", metrics)\n",
    "lambdamart_df = get_baseline_df(\"storage/clueweb12_3.0/Fold{}/vali.txt\", \"storage/baseline/scores/lambdamart_{}\", metrics)\n",
    "adarank_df = get_baseline_df(\"storage/clueweb12_3.0/Fold{}/vali.txt\", \"storage/baseline/scores/adarank_{}\", metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('p@1', 0.08960816958126622),\n",
       " ('p@5', 0.00963157804344407),\n",
       " ('p@10', 0.07182853967570887),\n",
       " ('ndcg@1', 0.15789906902414283),\n",
       " ('ndcg@5', 0.013354235829421737),\n",
       " ('ndcg@10', 0.028756056417114255),\n",
       " ('map', 0.8033449468254168)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_test( vis_dfs[0], lambdamart_df, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "query_id    250.500000\n",
       "p@1           0.338000\n",
       "p@5           0.358800\n",
       "p@10          0.370000\n",
       "ndcg@1        0.189167\n",
       "ndcg@5        0.215016\n",
       "ndcg@10       0.232574\n",
       "map           0.414786\n",
       "dtype: float64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lambdamart_df = get_baseline_df(\"storage/clueweb12_3.0/Fold{}/test.txt\", \"storage/baseline/scores/lambdamart_P@5_{}\", metrics)\n",
    "vis_dfs[0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "query_id    250.690000\n",
       "p@1           0.440000\n",
       "p@5           0.442000\n",
       "p@10          0.467000\n",
       "ndcg@1        0.242500\n",
       "ndcg@5        0.268343\n",
       "ndcg@10       0.293948\n",
       "map           0.433493\n",
       "dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambdamart_same_df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "query_id    250.500000\n",
       "p@1           0.560000\n",
       "p@5           0.546800\n",
       "p@10          0.519800\n",
       "ndcg@1        0.322500\n",
       "ndcg@5        0.336874\n",
       "ndcg@10       0.346342\n",
       "map           0.455765\n",
       "dtype: float64"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vis_dfs[5].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
