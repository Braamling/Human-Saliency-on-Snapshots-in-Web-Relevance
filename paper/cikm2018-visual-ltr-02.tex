% !TEX root = cikm2018-visual-ltr.tex

\section{Related work}
\label{sec:relatedwork}

We discuss related research on
\begin{inparaenum}[(i)]
\item the relation between the visual appearance of a web page and how it is perceived by users, and
\item the usage of visual information in \ac{LTR}.
\end{inparaenum} 

%The work in the paper is related to various research on user experience design. In this section we will discuss research on:
%\begin{inparaenum}[(i)]
%\item measuring usability,  
%\item predicting saliency on web pages, and 
%\item visual features in \ac{LTR}.
%\end{inparaenum} 

Using eye-tracking, \citet{nielsen2006f} and \citet{pernice2017f} demonstrate that web page design and content placement influences the ability for users to find the information they are looking for. 
Both studies show that by organizing the content in certain shapes (i.e., an F-shape), information can be  navigated more efficiently.
\citet{wang2014eye} show that the size of the fixation areas measured using eye-tracking is larger on web pages with more content, which increases the likelihood that the attention of a user is distracted.
%Finally, \citet{lindgaard2006attention} show that users are able to construct a stable judgment of a web page's visual appeal within 50ms. \todo{Is this important at all? I would drop the last reference and maybe use one or two others.}
Such studies show the importance of the visual appearance of a web page and its effect on how users perceive pages, demonstrating that visual information has to be taken into account when ranking web pages.

\citet{fan2017learning} are the first to approach this problem.
The authors use snapshots of web pages to extract visual features for LTR
and show that such visual features significantly improve retrieval performance.
\citet{fan2017learning} feed snapshots through a neural network that attempts to model the previously mentioned F-shape.
The output of this neural network is then concatenated with more traditional ranking signals, such as BM25 and PageRank.
Finally, the proposed model (called ViP) is trained end-to-end by using a pairwise loss.
%\todo{Here, we have to briefly repeat what we have said in the introduction:
%there are two limitations in \cite{fan2017learning} (what limitations?), which we fix in this paper (how do we do that?).} 

The work of \citet{fan2017learning} is limited by not using advanced visual feature extraction methods and by the GOV2 dataset, which lacks visual diversity.
We approach these problems by proposing an improved feature extraction method using both pre-trained weights from a deep convolutional network~\cite{simonyan2014very} as well as synthetic saliency heatmaps~\cite{shan2017two} as input images, and by introducing \datasetname, a more diverse dataset based on ClueWeb12.
% TODO reevaluate in which form we want to keep the lines above.

Recently, \citet{zhang2018relevance} propose a multimodal architecture for re-ranking \acp{SERP} by learning visual patterns in the result snippets. 
Their work demonstrates the potential of exploring visual patterns to improve \ac{LTR} performance. 
\todo{But how does it relate to our work?}

%\todo{Now that we have put saliency more to the foreground, we might want to reintroduce the text below}
%A number of techniques have been developed to predict saliency heatmaps on various images. \citet{buscher2009you} analyse the Web page's Document Object Model (DOM) to identify highly salient areas. More recent work from \citet{kummerer2016deepgaze} (on natural images) and \citet{shan2017two} (on web pages) use deep learning techniques to predict state-of-the-art saliency heatmaps. 

%In this work, we create a more generic approach by using synthetic generated saliency heatmaps.
%These heatmaps are used as an input to a convolution network in order to create features that can be used as an indicator of a web page communication effectiveness and usability. 

%\citet{donahue2014decaf} show that the features learned on large-scale supervised data can be transferred to different tasks and labels. Transferring the feature extraction weights to a new task is a common solution to cope with relatively small datasets. Most query sets used for LTR are relatively small (approximately 30,000 documents) compared to a dataset as ImageNet (1 million images), which indicates that transfer learning methods can be of use. In this work we use a pretrained image classifier and fine-tune its final layers on a LTR task. 

% Write something on how saliency can be used for evaluating web pages

% TODO: describe more related work.
% TODO: Show some work on visual features in web design
% TODO: Describe the visual paper that was accepted to CIKM