% !TEX root = cikm2018-visual-ltr.tex

\section{Visual ranking methods}
In this section we discuss the improved visual feature extraction methods that we use for \ac{LTR} based on the visual appearance of web pages. The corresponding code is available on Github.\footnote{URL removed for review}

\subsection{Visual feature models}
Similarly to \cite{fan2017learning}, we use a combination of visual and content features for training.
This is achieved by separating the base model into a visual feature extraction and scoring component.
The visual feature extraction component takes an image $x_{img}$ as an input and outputs a visual feature vector $x_{v}$.
This feature vector is concatenated with a content feature vector $x_{c}$ and the concatenated vector is then used as an input to the scoring component.
In our case, we use the same single fully connected layer with a hidden size of $10$ as by~\citep{fan2017learning}, but added a dropout of $10\%$ which was empirically found to improve the performance. 
The visual feature extraction and scoring components are trained end-to-end using a pairwise hinge loss with $L_2$ regularization as in~\cite{fan2017learning}.
For the experiments, we used various visual feature extraction models described below.

\paragraph{ViP visual features}
As a baseline, we implement the visual feature extractor proposed in~\citet{fan2017learning} using PyTorch.
In this model, an input image $x_{img}$ is gray-scaled, normalized and horizontally segmented into $4\times16$ slices, which are processed separately through a shallow convolutional network.
This output is then passed top to bottom through an LSTM which results in a visual feature vector~$x_{v}$. 
%The full dimensions can be found in Figure \ref{fig:ViPfeat}.

\paragraph{VGG-16 visual features}
Since the dataset in \citep{fan2017learning} and the \datasetname{} dataset that we introduce in this paper have a relatively low number of snapshots to train a separate feature extractor, we use the on ImageNet pretrained VGG-16 model~\cite{simonyan2014very} from Torchvision\footnote{\url{https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py}} as a feature extractor. Although VGG-16 is not the state-of-the-art visual feature extractor, it is a reasonable trade-off between effectiveness and simplicity.
The VGG-16 architecture consists of a set of convolutional filters and fully connected layers. The convolutional filters extract features from the input image, which are used by the fully connected layers to classify the image. 
These filters are generic with respect to input and task~\citep{donahue2014decaf}, so we can reuse them to create visual features for LTR. The fully connected layers can altered and retrained to specifically learn to handle new inputs and tasks. 
In our case, we replace the last fully connected layer with a reinitialized layer with an output of size $30$. During training, we only optimize the fully connected layers.

VGG-16 uses a $224\times224$ image with three color channels as input as opposed to gray-scaled $64\times64$ input used by ViP, giving us the advantage that both color and low level features are persisted. 


\paragraph{Saliency heatmaps}
Following \cite{shan2017two}, we generate synthetic saliency heatmaps for each snapshot in our data\-set. 
These heatmaps attempt to predict the viewing pattern of a user, creating a data driven alternative to the F-shape that is modeled in the ViP architecture. 
The heatmaps have a dimension of $64\times64$ which is linearly scaled to $224\times244$ in order to use them as input to the VGG-16 feature extraction component discussed above.
Fig.~\ref{fig:exampleshots} shows example snapshots with their corresponding saliency heatmaps.
