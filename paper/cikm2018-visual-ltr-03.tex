% !TEX root = cikm2018-visual-ltr.tex

\section{Visual feature extraction methods}
In this section we discuss the improved visual feature extraction methods that we use for \ac{LTR} based on the visual appearance of web pages. The corresponding code is available on Github.\footnote{URL removed for review}

%\subsection{Visual feature models}
Similarly to \cite{fan2017learning}, we use a combination of visual and content features for training.
This is achieved by separating the base model into a visual feature extraction and scoring component.
The visual feature extraction component takes an image $x_{img}$ as an input and outputs a visual feature vector $x_{v}$.
This feature vector is concatenated with a content feature vector $x_{c}$ and the concatenated vector is then used as an input to the scoring component.
In our case, we use the same single fully connected layer with a hidden size of $10$ as by~\citep{fan2017learning}, but added a dropout of $10\%$ which was empirically found to improve the performance. 
The visual feature extraction and scoring components are trained end-to-end using a pairwise hinge loss with $L_2$ regularization as in~\cite{fan2017learning}.
For the experiments, we used various visual feature extraction models described below.

\paragraph{ViP visual features}
As a baseline, we implement the visual feature extractor proposed in~\citet{fan2017learning} using PyTorch.
In this model, an input image $x_{img}$ is gray-scaled, normalized and horizontally segmented into $4\times16$ slices, which are processed separately through a shallow convolutional network.
This output is then passed top to bottom through an LSTM which results in a visual feature vector~$x_{v}$. 
%The full dimensions can be found in Figure \ref{fig:ViPfeat}.

\paragraph{VGG-16 visual features}
Since the dataset in \citep{fan2017learning} and the \datasetname{} dataset that we introduce in this paper have a relatively low number of snapshots to train a separate feature extractor, we use the VGG-16 model~\cite{simonyan2014very} from Torchvision,\footnote{\url{https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py}} pretrained on ImageNet, as a feature extractor. 
Although VGG-16 is not the state-of-the-art visual feature extractor, it is a reasonable trade-off between effectiveness and simplicity.
The VGG-16 architecture consists of a set of convolutional filters and fully connected layers. 
The convolutional filters extract features from the input image, which are used by the fully connected layers to classify the image. 
These filters are generic with respect to input and task~\citep{donahue2014decaf}, so we can reuse them to create visual features for LTR. 
The fully connected layers can be altered and retrained to specifically learn to handle new inputs and tasks. In order to increase training perforamnce and reduce the computational requirements we first transform all input images using the convolutional layers, changing all $3\times224\times224$ input images to a single vector of $1\times25088$ drastically reducing the amount of parameters. The remaining fully connected layers are retrained, with the last fully connected layer reinitalized to produce an output of size $30$.

% VGG-16 uses a $224\times224$ image with three color channels as input as opposed to the gray-scaled $64\times64$ input used by ViP, giving us the advantage that both color and low level features persist. 

\paragraph{ResNet-152 visual features}
The ResNet-152 architecture has a significant increase in image classification over VGG-16\cite{he2016deep}. The residual connections between convolutional layers allow for deeper networks to be trained without suffering from vanishing gradients. Similar as with the VGG-16 model, we use the convolutional layers to convert each $3\times224\times224$ input images to a single vectors of $1\times2048$. The original ResNet-152 architecture only has a single fully connected layer making it hard to fine tune the model. Instead, we use completely untrained fully connected network consisting of three layers with $4096$ hidden units, producing a final output of size $30$.


\paragraph{Saliency heatmaps}
Following \cite{shan2017two}, we generate synthetic saliency heatmaps for each snapshot in our data\-set. 
These heatmaps attempt to predict the viewing pattern of a user, creating a data driven alternative to the F-shape that is modeled in the ViP architecture. 
The heatmaps have a dimension of $64\times64$ which is linearly scaled to $224\times224$ in order to use them as input to the VGG-16 feature extraction component discussed above.
Fig.~\ref{fig:exampleshots} shows example snapshots with their corresponding saliency heatmaps.
