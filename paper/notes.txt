Improvements to the current version of the paper:
* Use state-of-the-art LTR methods for scoring (RankBoost, LambdaMart)
* Run experiments on GOV2, like Fan et al.
* Drop snapshots with highlights, they are not central and important
* Bring saliency forward and make it the main point and contribution of the paper (more on this below)

Questions about the ViTOR dataset:
* Do we publish visual features vectors (at least the generic ones)?
* Do we publish saliency heatmaps?

Some thoughts about saliency heatmaps:
* Extracting visual features from saliency heatmaps is clearly a novel approach in LTR with visual features
* It is fair to compare saliency heatmaps to vanilla snapshots, not the highlighted ones. In this comparison, heatmaps win.
* Moreover, they are way more efficient in terms of memory and processing time. To support this point, we need experiments with both memory and time.
* On the other hand, it is not at all clear to me, why saliency heatmaps are so effective.
   * Figure 2 shows that heatmaps for very different webpages look very similar. There seem to be a strong prior when creating heatmaps. For example, there is a lot of attention in the middle of the images. Then the attention spread towards the corners almost uniformly with some preference for the left upper corner. Given high similarity of heatmaps for different webpages, it is even more surprising they perform so well.
   * A qualitative analysis is really needed here to understand how saliency heatmaps are generated and what information they cary.
   * We need some sanity check experiment, where we pass a random snapshot into the ViTOR model. I would like to check whether it is really visual features that improve the LTR performance or something else.