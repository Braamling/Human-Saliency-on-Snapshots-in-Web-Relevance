% !TEX root = www2019-visual-ltr.tex

\section{Experimental Setup}\label{sec:setup}
The experiments are divided into two types of setups:
\begin{inparaenum}[(i)]
\item baseline experiments using only content features, and
\item visual experiments using both content and visual features.
\end{inparaenum}

The \modelname~baseline refers to the \modelname~model without any visual features, such that it demonstrates the performance increase of introducing visual features. We do so by feeding the content features into the scoring component directly, without adding any visual features. 
The other content baseline experiments are performed using BM25 and the RankLib\footnote{\url{https://sourceforge.net/p/lemur/wiki/RankLib/}} implementations of RankBoost, AdaRank, and LambdaMart.

As a baseline comparison for our multimodal model, we implement ViP, the visual feature extractor proposed in~\citet{fan2017learning} using PyTorch. We train ViP on both the vanilla and highlighted snapshot datasets with the results being referred to as ViP snapshots and ViP highlights, respectively.

The performance of the \modelname~model is demonstrated by learning visual feature vectors using VGG-16 and ResNet-152. Similarly as with the ViP model, both VGG-16 and ResNet-152 are trained on both the vanilla and highlighted snapshot dataset. 

Finally, we use the synthetic saliency heatmaps as an input to the \modelname~model. For the saliency heatmaps, we again use both VGG-16 and ResNet-152 to learn the visual feature vectors.

Each run is generated using the Adam optimizer with a batch size of $100$. The learning rates for all ResNet-152 and VGG-16 setups were set to $0.00005$ and $0.0001$, respectively. 

To measure the retrieval performance, we used precision and ndcg at $\{1,10\}$ and MAP.
Significance is determined using a two-tailed paired t-test (p-value $\leq 0.05$). 

All PyTorch experiments were performed on a single GTX 1080 Ti GPU with 11gb of RAM. 
% Preprocessing was performed on a Thinkpad X250 with an Intel i5-5300U CPU and 16gb of ram. 



\section{Results}
In this section, we present experiments that are set out to test the following:
\begin{inparaenum}[(i)]
    \item the \modelname~model improves the \ac{LTR} performance when introducing visual features, 
    \item synthetic saliency heatmaps improve the \ac{LTR} performance when used as an input the \modelname{} model, and
    \item the \modelname~model improves both visual and non-visual state-of-the-art ranking methods.
%    \item the results of the baseline ViP model~\cite{fan2017learning} are reproduced on the \datasetname{} dataset.
\end{inparaenum}

\subsection{\modelname~model with VGG-16 and ResNet-152}
In Table~\ref{tab:letorvisresults}, we compare the performance of the \modelname~model when used with and without visual features.
The first row shows the \modelname~baseline, when using only the content features as an input to the scoring component.
The second to fifth rows show the performance of using VGG-16 and ResNet-152 with both vanilla and highlighted snapshots. 
These results clearly show that both VGG-16 and ResNet-152 visual feature extraction methods significantly improve the performance compared to the \modelname~baseline. 

When comparing the results of the \modelname~model with visual features, we observe the following:
\begin{inparaenum}[(i)]
    \item The highest ranking performance is achieved by using VGG-16 on the highlighted snapshots.
    \item For VGG-16, the values of all metrics are consistently better for highlighted snapshots compared to vanilla snapshots, which is inline with the findings of~\cite{fan2017learning} and to be expected: highlighted snapshots cary more information compared to vanilla snapshots.
    \item For ResNet-152, however, p@1 and ndcg@1 are lower for highlighted snapshots compared to vanilla snapshots. \todo{Can we explain this? In not, then we should probably skip this line completely.}
%    \item the performance increase for p@1 and ndcg@1, with an average of $+0.200$ and $+0.118$, is consistently higher than p@10 and ndcg@10, with an average of $+0.134$ and $+0.087$.
\end{inparaenum}
Based on these results, we conclude that the use of visual features in \ac{LTR} significantly improves performance
and that highlighted snapshots should on average be preferred over vanilla snapshots.
%These results are inline with the findings of~\cite{fan2017learning}.

\begin{table}[h]
\caption{Results for the \modelname~model using only content features (baseline), vanilla snapshots, highlighted snapshots, and saliency heatmaps.
All results have a significant improvement over the \modelname~baseline.
Best results are shown in bold.}
\label{tab:letorvisresults}
\centering
\begin{tabular}{l\OK l\OK l\OK l\OK l\OK l}
\toprule
                      & p@1    & p@10  & ndcg@1  & ndcg@10 & MAP   \\ 
\midrule
\modelname~baseline & 0.338  & 0.370 & 0.189   & 0.233   & 0.415 \\ 
\midrule
VGG snapshots      & 0.514 & 0.484 & 0.292 & 0.324 & 0.442 \\ 
ResNet snapshots   & 0.550 & 0.452 & 0.310 & 0.301 & 0.437 \\ 
VGG highlights     & \textbf{0.560} & \textbf{0.520} & 0.323 & \textbf{0.346} & \textbf{0.456} \\ 
ResNet highlights  & 0.530 & 0.463 & 0.305 & 0.312 & 0.440 \\
\midrule
VGG saliency       & 0.554 & 0.453 & 0.310   & 0.302   & 0.422 \\ 
ResNet saliency    & \textbf{0.560} & 0.476 & \textbf{0.333} & 0.321 & 0.442 \\
\bottomrule
\end{tabular}
\end{table}


\subsection{\modelname~model with synthetic saliency heatmaps}
The last two rows of Table~\ref{tab:letorvisresults} show the performance of the \modelname{} mo\-del when using synthetic saliency heatmaps as an input.
Similarly to the vanilla and highlighted snapshots, the visual features are learned using both VGG-16 and ResNet-152.
In contract to the vanilla and highlighted snapshots, however, ResNet-152 seems to consistently outperform VGG-16 when using synthetic saliency heatmaps.
Although the highlighted snapshots with VGG-16 still outperform ResNet-152 with saliency heatmaps on p@10, ndcg@10 and MAP, the saliency heatmaps with ResNet-152 match and outperform VGG-16 with highlighted snapshots when looking at p@1 and ndcg@1.
\todo{Can we say anything about statistical significance in this subsection?}
\todo{Can we explain these results?}
Based on these results, we conclude that saliency heatmaps should be preferred in applications where early precision is important,
while highlighted snapshots should be used when a high overall performance is needed.


\subsection{Baseline comparison}
Table~\ref{tab:baseresults} compares the performance of the \modelname~model to BM25, non-visual \ac{LTR} methods and the ViP model by~\citet{fan2017learning}. 
Specifically, the table shows the performance of VGG-16 with highlighted snapshots and of ResNet-152 with synthetic saliency heatmaps, as these are the best-performing variants of the \modelname~model according to Table~\ref{tab:letorvisresults}.
%Because these two results yield the highest \ac{LTR} performance, they are used to compare the \modelname~model with the baselines. 
We clearly see that both methods have a significant performance increase compared to BM25, almost doubling the metrics values in many cases.

When comparing to non-visual \ac{LTR} methods, both \modelname~implementations show consistently better performance.
However, not all metrics are improved significantly.
%However, we do find a significant improvement by the \modelname~implementations on the AdaRank \ac{LTR} results.
%Furthermore, when using VGG-16 with highlighted snapshots there is a significant performance increase in ndcg@10 compared to LambdaMart and in p@10 for both RankBoost and LambdaMart.
We attribute this to the fact that, similarly to~\cite{fan2017learning}, the \ac{LTR} component of the \modelname{} model is based on pairwise hinge loss, which is a relatively simple loss function.
As a future work, we plan to investigate the effect of different loss functions on the performance of the \modelname{} model.

Finally, we compare the \modelname~implementations to ViP, the only existing \ac{LTR} method with visual features.
Here, we clearly see that both our implementations significantly outperform ViP on all metrics.

The above results show that the proposed \modelname{} model outperforms baselines, whether they are supervised or unsupervised, use visual features or not.
However, to achieve consistent significant improvements compared to the state-of-the-art \ac{LTR} methods, different loss functions within the \modelname{} model have to be investigated.


\begin{table}[h]
\caption{Results for the VGG-16 with highlighted snapshots, ResNet-152 with saliency heatmaps, and baselines.
$\dagger$ indicates a significant decrease in performance compared to VGG highlights and $\ddagger$ indicates a significant decrease in performance compared to both \modelname{} implementations.
Best results are shown in bold.}

\label{tab:baseresults}
\begin{tabular}{l\OK l\OK l\OK l\OK l\OK l}
\toprule
                      & p@1    & p@10  & ndcg@1  & ndcg@10 & MAP   \\
\midrule
BM25                  & 0.300$^\ddagger$  & 0.316$^\ddagger$ & 0.153$^\ddagger$   & 0.188$^\ddagger$   & 0.350$^\ddagger$ \\ 
\midrule
RankBoost             & 0.450  & 0.444 & 0.258   & 0.288$^\dagger$    & 0.427 \\
AdaRank               & 0.290$^\ddagger$   & 0.357$^\ddagger$  & 0.149$^\ddagger$    & 0.227$^\ddagger$    & 0.398 \\
LambdaMart            & 0.470  & 0.420$^\dagger$ & 0.256   & 0.275$^\dagger$    & 0.418 \\ 
\midrule
ViP snapshots         & 0.392$^\ddagger$ & 0.398$^\ddagger$ & 0.217$^\ddagger$   & 0.254$^\ddagger$   & 0.421$^\ddagger$ \\ 
ViP highlights        & 0.418$^\ddagger$  & 0.416$^\ddagger$ & 0.239$^\ddagger$   & 0.269$^\ddagger$   & 0.422$^\ddagger$ \\
\midrule
VGG highlights        & \textbf{0.560}  & \textbf{0.520} & 0.323   & \textbf{0.346}   & \textbf{0.456} \\ 
ResNet saliency       & \textbf{0.560} & 0.476 & \textbf{0.333} & 0.321 & 0.442 \\
\bottomrule
\end{tabular}
\end{table}



