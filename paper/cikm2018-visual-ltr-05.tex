% !TEX root = www2019-visual-ltr.tex

\section{Experimental Setup}\label{sec:setup}
In this section, we discuss the configurations of the \modelname~architecture, baselines and metrics used during our experiments.

%The experiments are divided into two types of setups:
%\begin{inparaenum}[(i)]
%\item baseline experiments using only content features, and
%\item visual experiments using both content and visual features.
%\end{inparaenum}

\paragraph{\modelname~configurations}
We experiment with four configurations of the \modelname~architecture.
\modelname~baseline refers to the \modelname~model with only content features.
This configuration is trained by feeding content features into the scoring component directly, without adding any visual features.
\modelname~snapshots and \modelname~highlights use visual features extracted from vanilla snapshots of webpages and from snapshots of webpages with highlighted query terms, respectively.
Finally, \modelname~saliency uses visual features extracted from synthetic saliency heatmaps.

For the \modelname~configurations with visual features, we experiment with both VGG-16 and ResNet-152 visual feature extraction methods.
The learning rates for VGG-16 and ResNet-152 are set to and $0.0001$ and $0.00005$, respectively. 
Each experimental run is generated using the Adam optimizer~\cite{kingma2014adam} with a batch size of $100$.
These parameters are chosen based on preliminary experiments.


\paragraph{Baselines}
We compare our proposed \modelname~model to the ViP model by~\citet{fan2017learning}, the only existing \ac{LTR} method that uses visual features.
We train ViP on both vanilla and highlighted snapshots with the resulting configurations being ViP snapshots and ViP highlights, respectively.

Following~\cite{fan2017learning}, we also compare the \modelname~model to a number of content-based ranking methods,
namely BM25 and state-of-the-art \ac{LTR} techniques, such as RankBoost, AdaRank, and LambdaMart.\footnote{The \ac{LTR} methods are taken from \url{https://sourceforge.net/p/lemur/wiki/RankLib}.}


\paragraph{Metrics}
To measure the retrieval performance, we use precision and ndcg at $\{1,10\}$ and MAP.
Statistical significance is determined using a two-tailed paired t-test (p-value $\leq 0.05$). 

%All PyTorch experiments were performed on a single GTX 1080 Ti GPU with 11gb of RAM. 
% Preprocessing was performed on a Thinkpad X250 with an Intel i5-5300U CPU and 16gb of ram. 



\section{Results}
In this section, we present experiments that are set out to test the following:
\begin{inparaenum}[(i)]
    \item the \modelname~model improves the \ac{LTR} performance when introducing visual features, 
    \item synthetic saliency heatmaps improve the \ac{LTR} performance when used as an input the \modelname{} model, and
    \item the \modelname~model improves both visual and non-visual state-of-the-art ranking methods.
%    \item the results of the baseline ViP model~\cite{fan2017learning} are reproduced on the \datasetname{} dataset.
\end{inparaenum}

\subsection{\modelname~model with VGG-16 and ResNet-152}
In Table~\ref{tab:letorvisresults}, we compare the performance of the \modelname~model when used with and without visual features.
The first row shows the \modelname~baseline, when using only the content features as an input to the scoring component.
The second to fifth rows show the performance of using VGG-16 and ResNet-152 with both vanilla and highlighted snapshots. 
These results clearly show that both VGG-16 and ResNet-152 visual feature extraction methods significantly improve the performance compared to the \modelname~baseline. 

When comparing the results of the \modelname~model with visual features, we observe the following:
\begin{inparaenum}[(i)]
    \item The highest ranking performance is achieved by using VGG-16 on the highlighted snapshots.
    \item For VGG-16, the values of all metrics are consistently better for highlighted snapshots compared to vanilla snapshots, which is in line with the findings of~\cite{fan2017learning} and is to be expected: highlighted snapshots carry more information compared to vanilla snapshots.
\end{inparaenum}
Based on these results, we conclude that the use of visual features in \ac{LTR} significantly improves performance
and that highlighted snapshots should on average be preferred over vanilla snapshots.
%These results are in line with the findings of~\cite{fan2017learning}.

\begin{table}[h]
\caption{Results for the \modelname~model using only content features (baseline), vanilla snapshots, highlighted snapshots, and saliency heatmaps.
All results have a significant improvement over the \modelname~baseline.
Best results are shown in bold.}
\label{tab:letorvisresults}
\centering
\begin{tabular}{l\OK l\OK l\OK l\OK l\OK l}
\toprule
                      & p@1    & p@10  & ndcg@1  & ndcg@10 & MAP   \\ 
\midrule
\modelname~baseline & 0.338  & 0.370 & 0.189   & 0.233   & 0.415 \\ 
\midrule
VGG snapshots      & 0.514 & 0.484 & 0.292 & 0.324 & 0.442 \\ 
ResNet snapshots   & 0.550 & 0.452 & 0.310 & 0.301 & 0.437 \\ 
VGG highlights     & \textbf{0.560} & \textbf{0.520} & 0.323 & \textbf{0.346} & \textbf{0.456} \\ 
ResNet highlights  & 0.530 & 0.463 & 0.305 & 0.312 & 0.440 \\
\midrule
VGG saliency       & 0.554 & 0.453 & 0.310   & 0.302   & 0.422 \\ 
ResNet saliency    & \textbf{0.560} & 0.476 & \textbf{0.333} & 0.321 & 0.442 \\
\bottomrule
\end{tabular}
\end{table}


\subsection{\modelname~model with synthetic saliency heatmaps}
The last two rows of Table~\ref{tab:letorvisresults} show the performance of the \modelname{} mo\-del when using synthetic saliency heatmaps as an input.
The visual features are learned using both VGG-16 and ResNet-152.
In this case, ResNet-152 consistently outperforms VGG-16.
Although the highlighted snapshots with VGG-16 still outperform ResNet-152 with saliency heatmaps on p@10, ndcg@10 and MAP, the saliency heatmaps with ResNet-152 match and outperform VGG-16 with highlighted snapshots when looking at p@1 and ndcg@1.
% \todo{Can we say anything about statistical significance in this subsection? }
% \todo{Can we explain these results? No.}
Based on these results, we conclude that saliency heatmaps should be preferred in applications where early precision is important,
while highlighted snapshots should be used when a high overall performance is needed.


\subsection{Baseline comparison}
Table~\ref{tab:baseresults} compares the performance of the \modelname~model to BM25, non-visual \ac{LTR} methods and the ViP model by~\citet{fan2017learning}. 
Specifically, the table shows the performance of VGG-16 with highlighted snapshots and of ResNet-152 with synthetic saliency heatmaps, as these are the best-performing variants of the \modelname~model according to Table~\ref{tab:letorvisresults}.
%Because these two results yield the highest \ac{LTR} performance, they are used to compare the \modelname~model with the baselines. 
We clearly see that both methods have a significant performance increase compared to BM25, almost doubling the metrics values in many cases.

When comparing to non-visual \ac{LTR} methods, both \modelname~implementations show consistently better performance.
However, not all metrics are improved significantly.
%However, we do find a significant improvement by the \modelname~implementations on the AdaRank \ac{LTR} results.
%Furthermore, when using VGG-16 with highlighted snapshots there is a significant performance increase in ndcg@10 compared to LambdaMart and in p@10 for both RankBoost and LambdaMart.
We attribute this to the fact that, similarly to~\cite{fan2017learning}, the \ac{LTR} component of the \modelname{} model is based on pairwise hinge loss, which is a relatively simple loss function.
As a future work, we plan to investigate the effect of different loss functions on the performance of the \modelname{} model.

Finally, we compare the \modelname~implementations to ViP, the only existing \ac{LTR} method with visual features.
Here, we clearly see that both our implementations significantly outperform ViP on all metrics.
Also note, that ViP loses to two out of three non-visual \ac{LTR} baselines, namely RankBoost and LambdaMart.
We believe this is due to the reason discussed above: ViP uses pairwise hinge loss as the \ac{LTR} component~\cite{fan2017learning}, which may be suboptimal.

The above results show that the proposed \modelname{} model outperforms baselines, whether they are supervised or unsupervised, use visual features or not.
However, to achieve consistent significant improvements compared to the state-of-the-art \ac{LTR} methods, different loss functions within the \modelname{} model have to be investigated.


\begin{table}[h]
\caption{Results for the VGG-16 with highlighted snapshots, ResNet-152 with saliency heatmaps, and baselines.
$\dagger$ indicates a significant decrease in performance compared to VGG highlights and $\ddagger$ indicates a significant decrease in performance compared to both \modelname{} implementations.
Best results are shown in bold.}

\label{tab:baseresults}
\begin{tabular}{l\OK l\OK l\OK l\OK l\OK l}
\toprule
                      & p@1    & p@10  & ndcg@1  & ndcg@10 & MAP   \\
\midrule
BM25                  & 0.300$^\ddagger$  & 0.316$^\ddagger$ & 0.153$^\ddagger$   & 0.188$^\ddagger$   & 0.350$^\ddagger$ \\ 
\midrule
RankBoost             & 0.450  & 0.444 & 0.258   & 0.288$^\dagger$    & 0.427 \\
AdaRank               & 0.290$^\ddagger$   & 0.357$^\ddagger$  & 0.149$^\ddagger$    & 0.227$^\ddagger$    & 0.398 \\
LambdaMart            & 0.470  & 0.420$^\dagger$ & 0.256   & 0.275$^\dagger$    & 0.418 \\ 
\midrule
ViP snapshots         & 0.392$^\ddagger$ & 0.398$^\ddagger$ & 0.217$^\ddagger$   & 0.254$^\ddagger$   & 0.421$^\ddagger$ \\ 
ViP highlights        & 0.418$^\ddagger$  & 0.416$^\ddagger$ & 0.239$^\ddagger$   & 0.269$^\ddagger$   & 0.422$^\ddagger$ \\
\midrule
VGG highlights        & \textbf{0.560}  & \textbf{0.520} & 0.323   & \textbf{0.346}   & \textbf{0.456} \\ 
ResNet saliency       & \textbf{0.560} & 0.476 & \textbf{0.333} & 0.321 & 0.442 \\
\bottomrule
\end{tabular}
\end{table}




