% !TEX root = cikm2018-visual-ltr.tex

%\section{Experimental Setup}\label{sec:experiments}
%In this section, we discuss the setup of the experiments performed on \datasetname. The experiments are set out to demonstrate the abilities of visual features in LTR and set a baseline for future visual LTR research.
%%All PyTorch experiments were performed on a GTX 1080 Ti with 11gb of RAM. Preprocessing was performed on a Thinkpad X250 with an Intel i5-5300U CPU and 16gb of ram. 
%
%\paragraph{\ac{LTR} with visual features}
%To average out the difference caused by random initialization, the experiments on \ac{LTR} with visual features are run five times for each of the five folds. 
%Each run is performed using the Adam optimizer with a batch size of 100 using a learning rate of $0.0001$ and $0.00005$ for the ViP and VGG-16 models respectively. These values were found empirically to find the highest performance for each model.
%Within each fold, the results for all five runs are averaged to create one measurement per fold per query.

% !TEX root = cikm2018-visual-ltr.tex


\section{Experiments and Results}
In this section, we discuss experiments performed on the proposed \datasetname~dataset.
These experiments are set out to test whether:
\begin{inparaenum}[(i)]
\item the \modelname~model can improve the \ac{LTR} performance when introducing visual feature vectors. 
\item synthetic saliency heatmaps improve the \ac{LTR} performance when used as an input the \modelname{} model, and
\item the \modelname~model results can improve both visual and non-visual state-of-the-art \ac{LTR} methods.
\end{inparaenum}

\subsection{Experimental Setup}
The experiments are divided into two types of setups:
\begin{inparaenum}[(i)]
\item baseline experiments using only content features, and
\item visual experiments using both content and visual features.
\end{inparaenum}

The \modelname~baseline refers to the baseline that demonstrates the performance increase of introducing visual features. We do so by using the \modelname~model and feeding the content features into the scoring component directly, without adding any visual features. 
The other content baseline experiments are performed using BM25 and the RankLib\footnote{\url{https://sourceforge.net/p/lemur/wiki/RankLib/}} implementations of RankBoost, AdaRank, and LambdaMart.

As a baseline comparison for our multimodal model, we implement ViP, the visual feature extractor proposed in~\citet{fan2017learning} using PyTorch. We train ViP on both the vanilla and highlighted snapshot datasets with the results being referred to as ViP snapshots and ViP highlights respectively.

The performance of the \modelname~model is demonstrated by learning visual feature vectors using VGG-16 and ResNet-152. Similarly as with the ViP model, both VGG-16 and ResNet-152 are trained on both the vanilla and highlighted snapshot dataset. 

Finally, we use the synthetic saliency heatmaps as an input to the \modelname~model. For the saliency heatmaps, we again use both VGG-16 and ResNet-152 to learn the visual feature vectors.

Each run is generated using the Adam optimizer with a batch size of $100$. The learning rates for all ResNet-152 and VGG-16 setups were set to $0.00005$ and $0.0001$ respectively. We empirically found that these values yield the highest performance for each model.

To measure the retrieval performance, we used precision and ndcg at $\{1,10\}$ and MAP.
Significance is determined using a two-tailed paired t-test (p-value $\leq 0.05$). 

All PyTorch experiments were performed on a single GTX 1080 Ti GPU with 11gb of RAM. 
% Preprocessing was performed on a Thinkpad X250 with an Intel i5-5300U CPU and 16gb of ram. 


\subsection{Results}
In this section we will compare and discuss the results of the 
\begin{inparaenum}[(i)]
\item \modelname~implementions, 
\item non-visual baselines, and 
\item visual baselines.
\end{inparaenum}  

\paragraph{\modelname~model with VGG-16 and ResNet-152}
In Table~\ref{tab:letorvisresults}, we compare the performance of the \modelname~model when used with and without visual feature vectors. The first row shows the \modelname~baseline, when using only the content features as an input to the scoring component. The second to fifth rows shows the performance of using VGG-16 and ResNet-152 with both vanilla and highlighted snapshots. 
These results clearly show that each of the methods of learning visual feature vectors significantly improves the \ac{LTR} performance of the \modelname~model compared to the \modelname~baseline. 

When comparing the results of using visual feature vectors we observe the following:
\begin{inparaenum}[(i)]
\item the highest ranking performance is achieved by using VGG-16 on the highlighted snapshots, 
\item performance on all performance metrics is consistently better when comparing highlighted with vanilla snapshots using VGG-16, which is not the case when comparing the two datasets using ResNet-152, and 
\item the performance increase for p@1 and ndcg@1, with an average of $+0.200$ and $+0.118$, is consistently higher than p@10 and ndcg@10, with an average of $+0.134$ and $+0.087$.
\end{inparaenum}

\begin{table}[h]
\caption{The \ac{LTR} results for the \modelname~model using vanilla snapshots, highlighted snapshots, saliency heatmaps and content features only. All results have a significant improvement over the \modelname~baseline. }
\label{tab:letorvisresults}
\centering
\begin{tabular}{l\OK l\OK l\OK l\OK l\OK l}
\toprule
                      & p@1    & p@10  & ndcg@1  & ndcg@10 & MAP   \\ 
\midrule
\modelname~baseline & 0.338  & 0.370 & 0.189   & 0.233   & 0.415 \\ 
\midrule
VGG snapshots      & 0.514 & 0.484 & 0.292 & 0.324 & 0.442 \\ 
ResNet snapshots   & 0.550 & 0.452 & 0.310 & 0.301 & 0.437 \\ 
VGG highlights     & \textbf{0.560} & \textbf{0.520} & 0.323 & \textbf{0.346} & \textbf{0.456} \\ 
ResNet highlights  & 0.530 & 0.463 & 0.305 & 0.312 & 0.440 \\
\midrule
VGG saliency       & 0.554 & 0.453 & 0.310   & 0.302   & 0.422 \\ 
ResNet saliency    & \textbf{0.560} & 0.476 & \textbf{0.333} & 0.321 & 0.442 \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{\modelname~model with synthetic saliency heatmaps}
The last two rows of Table~\ref{tab:letorvisresults} shows the \ac{LTR} performance when using synthetic saliency heatmaps as an input to the \modelname~model. Similarly as the vanilla and higlighted snapshots, the visual feature vectors are learned by using both VGG-16 and ResNet-152. In contract to the vanilla and highlighted snapshots, ResNet-152 seems to consistently outperform VGG-16 when using synthetic saliency images. Although the highlighted snapshots with VGG-16 still outperform ResNet-152 with synthetic saliency images on p@10, ndcg@10 and MAP, the synthetic saliency images match and outperform performance when looking at p@1 and ndcg@1, respectively. 

\paragraph{Baseline comparison}
Table~\ref{tab:baseresults} compares the performance of the \modelname~model compared to both BM25, non-visual \ac{LTR} methods and the ViP model by~\citet{fan2017learning}. 
The table also shows both VGG-16 with highlighted snapshots and ResNet-152 with synthetic saliency heatmaps. Because these two results yield the highest \ac{LTR} performance, they are used to compare the \modelname~model with the baselines. 
We clearly see that both methods have a significant performance increase compared to using BM25, almost doubling the \ac{LTR} metrics.
Although both \modelname~implementations seem to consistently outperform all non-visual \ac{LTR} methods, we cannot confirm the statistical significance of each of the performance measures. 
However, we do find a significant improvement by the \modelname~implementations on the AdaRank \ac{LTR} results. Furthermore, when using VGG-16 with highlighted snapshots there is a significant performance increase in ndcg@10 compared to LambdaMart and in p@10 for both RankBoost and LambdaMart.

Finally, we compare the results of the \modelname~implementations with ViP as a visual \ac{LTR} baseline. 
Here we clearly see that both our implementations significantly outperform ViP on all measures. 

\paragraph{ViP results on \datasetname}
As addressed in the introduction, both the ViP model and the dataset used by~\citet{fan2017learning} have a number of limitations. The results in Table~\ref{tab:baseresults} clearly show that the limitations in the ViP model become apparent when being used with the more diverse and rich \datasetname~dataset. We do see that the results show a similar pattern as described by ~\citet{fan2017learning} where the model performs better when using highlighted snapshots compared to vanilla snapshots. However, using ViP with vanilla and highlighted snapshots from the \datasetname~dataset is outperformed by both RankBoost and LambdaMart. 

\begin{table}[h]
\caption{The $\dagger$ indicates a significant decrease in performance compared to the VGG-16 model with highlighted snapshots and $\ddagger$ indicates a significant decrease in performance for both \modelname~model results.}

\label{tab:baseresults}
\begin{tabular}{l\OK l\OK l\OK l\OK l\OK l}
\toprule
                      & p@1    & p@10  & ndcg@1  & ndcg@10 & MAP   \\
\midrule
BM25                  & 0.300$^\ddagger$  & 0.316$^\ddagger$ & 0.153$^\ddagger$   & 0.188$^\ddagger$   & 0.350$^\ddagger$ \\ 
\midrule
RankBoost             & 0.450  & 0.444 & 0.258   & 0.288$^\dagger$    & 0.427 \\
AdaRank               & 0.290$^\ddagger$   & 0.357$^\ddagger$  & 0.149$^\ddagger$    & 0.227$^\ddagger$    & 0.398 \\
LambdaMart            & 0.470  & 0.420$^\dagger$ & 0.256   & 0.275$^\dagger$    & 0.418 \\ 
\midrule
ViP snapshots         & 0.392$^\ddagger$ & 0.398$^\ddagger$ & 0.217$^\ddagger$   & 0.254$^\ddagger$   & 0.421$^\ddagger$ \\ 
ViP highlights        & 0.418$^\ddagger$  & 0.416$^\ddagger$ & 0.239$^\ddagger$   & 0.269$^\ddagger$   & 0.422$^\ddagger$ \\
\midrule
VGG highlights        & \textbf{0.560}  & \textbf{0.520} & 0.323   & \textbf{0.346}   & \textbf{0.456} \\ 
ResNet saliency       & \textbf{0.560} & 0.476 & \textbf{0.333} & 0.321 & 0.442 \\
\bottomrule
\end{tabular}
\end{table}



