% !TEX root = cikm2018-visual-ltr.tex

%\section{Experimental Setup}\label{sec:experiments}
%In this section, we discuss the setup of the experiments performed on \datasetname. The experiments are set out to demonstrate the abilities of visual features in LTR and set a baseline for future visual LTR research.
%%All PyTorch experiments were performed on a GTX 1080 Ti with 11gb of RAM. Preproccessing was performed on a Thinkpad X250 with an Intel i5-5300U CPU and 16gb of ram. 
%
%\paragraph{\ac{LTR} with visual features}
%To average out the difference cause by random initialization, the experiments on \ac{LTR} with visual features are run five times for each of the five folds. 
%Each run is performed using the Adam optimizer with a batch size of 100 using a learning rate of $0.0001$ and $0.00005$ for the ViP and VGG-16 models respectively. These values were found empirically to find the highest performance for each model.
%Within each fold, the results for all five runs are averaged to create one measurement per fold per query.

% !TEX root = cikm2018-visual-ltr.tex

\begin{table}[h]
\caption{Comparison between different \ac{LTR} methods with visual features. $\dagger$ indicates a significant improvement on ViP baseline and $\ddagger$ indicates an improvement on ViP highlights.}
\label{tab:visresults}
\centering
\begin{tabular}{llllll}
\toprule
                      & p@1    & p@10  & ndcg@1  & ndcg@10 & MAP   \\ 
\midrule
ViP baseline          & 0.338  & 0.370 & 0.189   & 0.233   & 0.415 \\ 
\midrule
ViP snapshots         & 0.392$^\dagger$ & 0.398$^\dagger$ & 0.217   & 0.254$^\dagger$   & 0.421 \\ 
ViP highlights        & 0.418$^\dagger$  & 0.416$^\dagger$ & 0.239$^\dagger$   & 0.269$^\dagger$   & 0.422 \\
\midrule
VGG snapshots      & 0.514$^\ddagger$    & 0.484$^\ddagger$ & 0.292$^\ddagger$   & 0.324$^\ddagger$   & 0.442$^\ddagger$ \\ 
VGG highlights     & 0.560$^\ddagger$    & 0.520$^\ddagger$ & 0.323$^\ddagger$   & 0.346$^\ddagger$   & 0.456$^\ddagger$ \\ 
\midrule
VGG saliency       & 0.554$^\ddagger$    & 0.453$^\ddagger$ & 0.310$^\ddagger$   & 0.302$^\ddagger$   & 0.422 \\ 
\bottomrule
\end{tabular}
\end{table}

\begin{table}[h]
\caption{Comparison of the VGG-16 model using highlights to content-based baselines. $*$ indicates a significant drop in performance compared to VGG highlights. }

\label{tab:baseresults}
\begin{tabular}{llllll}
\toprule
                      & p@1    & p@10  & ndcg@1  & ndcg@10 & MAP   \\
\midrule
BM25                  & 0.300$^*$  & 0.316$^*$ & 0.153$^*$   & 0.188$^*$   & 0.350$^*$ \\ 
\midrule
RankBoost             & 0.450  & 0.444 & 0.258   & 0.288$^*$    & 0.427 \\
AdaRank               & 0.290$^*$   & 0.357$^*$  & 0.149$^*$    & 0.227$^*$    & 0.398 \\
LambdaMart            & 0.470  & 0.420$^*$ & 0.256   & 0.275$^*$    & 0.418 \\ 
\midrule
VGG highlights        & 0.560  & 0.520 & 0.323   & 0.346   & 0.456 \\ 
\bottomrule
\end{tabular}
\end{table}


\section{Experiments and Results}
In this section, we discuss experiments performed on the proposed \datasetname~dataset.
These experiments are set out to demonstrate the abilities of visual features in LTR and set a baseline for future visual LTR research.

\if0
Table \ref{tab:11vs46} shows the results of the benchmark experiments. Each model was trained by optimizing the $P@5$ on the validation set. The results show that using the features from \datasetname~compared to the feature from LETOR has a minor impact on \ac{LTR} performance.
\fi

\paragraph{Experimental setup}
The experiments are divided into two types of setups:
\begin{inparaenum}[(i)]
\item baselines experiments using only content features, and
\item visual experiments using both content and visual features.
\end{inparaenum}

The ViP baseline refers to the experiments were the scoring component was used with only the content features. The other baseline experiments are performed using BM25, RankBoost, AdaRank and Lambdamart. 
The visual experiments are divided in the setups using the ViP feature extraction model and the VGG-16 extraction model. The input is defined as either the vanilla snapshots, red highlighted snapshots or saliency heatmap. 

To average out the difference cause by random initialization, the experiments on \ac{LTR} with visual features are run five times for each of the five folds. 
Each run is performed using the Adam optimizer with a batch size of 100 using a learning rate of $0.0001$ and $0.00005$ for the ViP and VGG-16 models respectively. We empirically found that these values yield the highest performance for each model.
Within each fold, the results for all five runs are averaged to create one measurement per fold per query.

We use the following retrieval performance measures: precision and NDCG at $\{1,10\}$ and MAP.
Significance is determined using a two tailed paired t-test (p-value $\leq 0.05$). 

\paragraph{Reproducing ViP}
Table~\ref{tab:visresults} shows the results using the different visual ranking methods and the ViP baseline. 
Results marked with a $\dagger$ have a significance improvement compared to the ViP baseline model. 
We see that in terms of significance the results are similar to the work of \citet{fan2017learning}, where the snapshots with highlights yield better performance than using the vanilla snapshots.
\todo{Can we say anything else here? (This is not necessary, just asking)}

\paragraph{Improved feature extraction methods}
The $\ddagger$ sign in Table~\ref{tab:visresults} indicates results where the VGG-16 model significantly outperforms the ViP model with highlighted snapshots. The results clearly show that introducing more powerful visual feature extraction methods significantly increases the \ac{LTR} performance.
\todo{Can we say anything else here? (This is not necessary, just asking)}

\paragraph{Content-based baselines}
Table \ref{tab:baseresults} compares the results of the VGG-16 model using highlighted snapshots with BM25, RankBoost, AdaRank and Lambdamart. The baselines with a significant difference from the VGG-16 model have been marked with a $*$. Although VGG-16 produces higher average results than the baseline methods, it is not possible to proof the statistical significance for some RankBoost and LambdaMart results. These results suggest that combining the visual features using more sophisticated ranking method could further improve \ac{LTR} performance using \datasetname. 

%\todo{Here we can add the benchmarking experiment that I left out for now.}

\paragraph{Benchmarking content features}
\todo{In fact, this should go into discussion and not experimental setup and/or experiments. But let's do this later.}
In \ac{LTR} research, the amount and type of content features used for training varies widely per dataset and study.
To assess the performance of the 11 content features used in this study (see Table~\ref{tab:setdescription}), we use a set of known LTR algorithms on the million query track 2007 (MQ2007)\cite{allan2007million} with the LETOR 4.0\footnote{\url{TODO}} content features.
MQ2007 and LETOR 4.0 have a total of 46 features containing all 11 features from Table~\ref{tab:setdescription}.
The experiment use the RankLib\footnote{https://sourceforge.net/p/lemur/wiki/RankLib/} implementations of RankBoost, AdaRank and LambdaMart on both 46 and 11 features using their default settings. 

\begin{table}[h]
\caption{Benchmark comparison on MQ2007 using all 46 LETOR features and the 11 LETOR features that are also used in \datasetname.}
\label{tab:11vs46}
\centering
\begin{tabular}{lccccc}
\toprule
%& \multicolumn{7}{c}{MQ2007 46 features vs 11 features}                                     \\
           & p@1  & p@10   & ndcg@1 & ndcg@10 & map \\ 
\midrule
RankBoost - 46 & 0.453 & 0.371 & 0.391 & 0.430  & 0.457 \\
RankBoost - 11 & 0.448 & 0.372 & 0.381  & 0.431   & 0.453 \\
\midrule
AdaRank - 46  & 0.420 & 0.360 & 0.367 & 0.424  & 0.449 \\
AdaRank - 11  & 0.385 & 0.287 & 0.364  & 0.394   & 0.386 \\ 
\midrule
LambdaMart - 46 & 0.452 & 0.384 & 0.405 & 0.444  & 0.463 \\
LambdaMart - 11 & 0.448 & 0.380 & 0.397  & 0.443   & 0.455 \\
\bottomrule
\end{tabular}
\end{table}