% !TEX root = cikm2018-visual-ltr.tex

%\section{Experimental Setup}\label{sec:experiments}
%In this section, we discuss the setup of the experiments performed on \datasetname. The experiments are set out to demonstrate the abilities of visual features in LTR and set a baseline for future visual LTR research.
%%All PyTorch experiments were performed on a GTX 1080 Ti with 11gb of RAM. Preproccessing was performed on a Thinkpad X250 with an Intel i5-5300U CPU and 16gb of ram. 
%
%\paragraph{\ac{LTR} with visual features}
%To average out the difference cause by random initialization, the experiments on \ac{LTR} with visual features are run five times for each of the five folds. 
%Each run is performed using the Adam optimizer with a batch size of 100 using a learning rate of $0.0001$ and $0.00005$ for the ViP and VGG-16 models respectively. These values were found empirically to find the highest performance for each model.
%Within each fold, the results for all five runs are averaged to create one measurement per fold per query.

% !TEX root = cikm2018-visual-ltr.tex

\newcommand{\OK}{@{\mbox{}\hspace*{.25cm}}}

\section{Experiments and Results}
In this section, we discuss experiments performed on the proposed \datasetname~dataset.
These experiments are set out to test whether:
\begin{inparaenum}[(i)]
\item the ViP results~\cite{fan2017learning} can be reproduced on \datasetname, 
\item commonly effective feature extractors can improve upon the ViP results, and
\item \ac{LTR} using visual features can outperform \ac{LTR} without visual features. \end{inparaenum}

\if0
Table \ref{tab:11vs46} shows the results of the benchmark experiments. Each model was trained by optimizing the $P@5$ on the validation set. The results show that using the features from \datasetname~compared to the feature from LETOR has a minor impact on \ac{LTR} performance.
\fi

\subsection{Experimental setup}
The experiments are divided into two types of setup:
\begin{inparaenum}[(i)]
\item baseline experiments using only content features, and
\item visual experiments using both content and visual features.
\end{inparaenum}

The ViP baseline refers to the experiments were the scoring component was used with only the content features. The other baseline experiments are performed using BM25 and the RankLib\footnote{\url{https://sourceforge.net/p/lemur/wiki/RankLib/}} implementations of RankBoost, AdaRank and LambdaMart.
The visual experiments are divided in the setups using the ViP feature extraction model and the VGG-16 feature extraction model. The input is defined as either the vanilla snapshots, red highlighted snapshots or saliency heatmap. 

To average out the difference caused by random initialization, the experiments on \ac{LTR} with visual features are run five times for each of the five folds. 
Each run is generated using the Adam optimizer with a batch size of 100 using a learning rate of $0.0001$ and $0.00005$ for the ViP and VGG-16 models, respectively. We empirically found that these values yield the highest performance for each model.
Within each fold, the results for all five runs are averaged to create one measurement per fold per query.

We use the following retrieval performance measures: precision and NDCG at $\{1,10\}$ and MAP.
Significance is determined using a two tailed paired t-test (p-value $\leq 0.05$). 

\subsection{Results}

\paragraph{Reproducing ViP}
The top part of Table~\ref{tab:visresults} shows the results using the different visual ranking methods and the ViP baseline. 
%Results marked with a $\dagger$ have a significance improvement compared to the ViP baseline model. 
We see that in terms of significance, the results and relative performance of the visual ranking methods are similar to the work of \citet{fan2017learning}, where the snapshots with highlights outperform using the vanilla snapshots. Although both snapshots with and without highlights have higher average results then the baseline, neither show a significant improvement in terms of MAP.

\begin{table}[h]
\caption{\ac{LTR} methods with visual features. $\dagger$ indicates a significant improvement over the ViP baseline and $\ddagger$ indicates a significant improvement over ViP highlights.}
\label{tab:visresults}
\centering
\begin{tabular}{l\OK l\OK l\OK l\OK l\OK l}
\toprule
                      & p@1    & p@10  & ndcg@1  & ndcg@10 & MAP   \\ 
\midrule
ViP baseline          & 0.338  & 0.370 & 0.189   & 0.233   & 0.415 \\ 
ViP snapshots         & 0.392$^\dagger$ & 0.398$^\dagger$ & 0.217   & 0.254$^\dagger$   & 0.421 \\ 
ViP highlights        & 0.418$^\dagger$  & 0.416$^\dagger$ & 0.239$^\dagger$   & 0.269$^\dagger$   & 0.422 \\
\midrule
VGG snapshots      & 0.514$^\ddagger$    & 0.484$^\ddagger$ & 0.292$^\ddagger$   & 0.324$^\ddagger$   & 0.442$^\ddagger$ \\ 
VGG highlights     & \textbf{0.560}$^\ddagger$    & \textbf{0.520}$^\ddagger$ & \textbf{0.323}$^\ddagger$   & \textbf{0.346}$^\ddagger$   & \textbf{0.456}$^\ddagger$ \\ 
VGG saliency       & 0.554$^\ddagger$    & 0.453$^\ddagger$ & 0.310$^\ddagger$   & 0.302$^\ddagger$   & 0.422 \\ 
\bottomrule
\end{tabular}
\end{table}


\paragraph{Improved feature extraction methods}
%The $\ddagger$ sign in Table~\ref{tab:visresults} indicates results where the VGG-16 model significantly outperforms the ViP model with highlighted snapshots.
The bottom part of Table~\ref{tab:visresults} shows that introducing more powerful visual feature extraction methods significantly increases the \ac{LTR} performance. Similarly as the ViP results, the performance is increased when query words in the snapshots are highlighted. The saliency heatmaps seem to only positively effect performance for the first few documents, with an improved p@1 and ndcg@1 compared to using vanilla snapshots, but having the same MAP score as ViP using highlights.

\paragraph{Content-based baselines}
Table \ref{tab:baseresults} compares the results of the VGG-16 model using highlighted snapshots with BM25, RankBoost, Ada\-Rank and Lambda\-Mart.
%The baselines with a significant drop in performance compared to the VGG-16 model have been marked with a $*$.
Although VGG-16 produces higher average results than the baseline methods, there is no statistically significant improvement over some of the RankBoost and LambdaMart results.


%These results suggest that by combining the visual features with m using more sophisticated ranking method could further improve \ac{LTR} performance using \datasetname.

\begin{table}[h]
\caption{VGG-16 model using highlights and content-based baselines. $*$ indicates a significant drop in performance compared to VGG highlights. }

\label{tab:baseresults}
\begin{tabular}{l\OK l\OK l\OK l\OK l\OK l}
\toprule
                      & p@1    & p@10  & ndcg@1  & ndcg@10 & MAP   \\
\midrule
BM25                  & 0.300$^*$  & 0.316$^*$ & 0.153$^*$   & 0.188$^*$   & 0.350$^*$ \\ 
\midrule
RankBoost             & 0.450  & 0.444 & 0.258   & 0.288$^*$    & 0.427 \\
AdaRank               & 0.290$^*$   & 0.357$^*$  & 0.149$^*$    & 0.227$^*$    & 0.398 \\
LambdaMart            & 0.470  & 0.420$^*$ & 0.256   & 0.275$^*$    & 0.418 \\ 
\midrule
VGG highlights        & \textbf{0.560}  & \textbf{0.520} & \textbf{0.323}   & \textbf{0.346}   & \textbf{0.456} \\ 
\bottomrule
\end{tabular}
\end{table}

%\todo{Here we can add the benchmarking experiment that I left out for now.}

\if0
\paragraph{Benchmarking content features}
\todo{In fact, this should go into discussion and not experimental setup and/or experiments. But let's do this later.}
In \ac{LTR} research, the amount and type of non-visual features used for training varies widely per dataset and study.
To assess the performance of the 11 non-visual features used in this study (see Table~\ref{tab:setdescription}), we use a set of known LTR algorithms on the million query track 2007 (MQ2007)\cite{allan2007million} with the LETOR 4.0\footnote{\url{https://www.microsoft.com/en-us/research/project/letor-learning-rank-information-retrieval/}} non-visual features.
MQ2007 and LETOR 4.0 have a total of 46 features containing all 11 features from Table~\ref{tab:setdescription}.
The experiment use the RankLib\footnote{https://sourceforge.net/p/lemur/wiki/RankLib/} implementations of RankBoost, AdaRank and LambdaMart on both 46 and 11 features using their default settings. 

\begin{table}[h]
\caption{Benchmark comparison on MQ2007 using all 46 LETOR features and the 11 LETOR features that are also used in \datasetname.}
\label{tab:11vs46}
\centering
\begin{tabular}{lccccc}
\toprule
%& \multicolumn{7}{c}{MQ2007 46 features vs 11 features}                                     \\
           & p@1  & p@10   & ndcg@1 & ndcg@10 & map \\ 
\midrule
RankBoost - 46 & 0.453 & 0.371 & 0.391 & 0.430  & 0.457 \\
RankBoost - 11 & 0.448 & 0.372 & 0.381  & 0.431   & 0.453 \\
\midrule
AdaRank - 46  & 0.420 & 0.360 & 0.367 & 0.424  & 0.449 \\
AdaRank - 11  & 0.385 & 0.287 & 0.364  & 0.394   & 0.386 \\ 
\midrule
LambdaMart - 46 & 0.452 & 0.384 & 0.405 & 0.444  & 0.463 \\
LambdaMart - 11 & 0.448 & 0.380 & 0.397  & 0.443   & 0.455 \\
\bottomrule
\end{tabular}
\end{table}

\fi