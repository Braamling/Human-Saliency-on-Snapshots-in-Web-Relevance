% !TEX root = www2019-visual-ltr.tex

\section{Results}
\label{sec:results}
In this section, we present experiments that are set out to test the following:
(1) the \modelname~model improves the \ac{LTR} performance when introducing visual features, 
(2) synthetic saliency heatmaps improve the \ac{LTR} performance when used as an input the \modelname{} model, and
(3) the \modelname~model improves both visual and non-visual state-of-the-art ranking methods.
%    \item the results of the baseline ViP model~\cite{fan2017learning} are reproduced on the \datasetname{} dataset.

\subsection{\modelname~model with VGG-16 and ResNet-152}
In Table~\ref{tab:letorvisresults}, we compare the performance of the \modelname~model when used with and without visual features.
The first row shows the \modelname~baseline, when using only the content features as an input to the scoring component.
The second to fifth rows show the performance of using VGG-16 and ResNet-152 with both vanilla and highlighted snapshots. 
These results clearly show that both VGG-16 and ResNet-152 visual feature extraction methods significantly improve the performance compared to the \modelname~baseline. 

When comparing the results of the \modelname~model with visual features, we observe the following:
(i) The highest ranking performance is achieved by using VGG-16 on the highlighted snapshots.
(ii) For VGG-16, the values of all metrics are consistently better for highlighted snapshots compared to vanilla snapshots, which is in line with the findings of~\cite{fan2017learning} and is to be expected: highlighted snapshots carry more information compared to vanilla snapshots.
Based on these results, we conclude that the use of visual features in \ac{LTR} significantly improves performance
and that highlighted snapshots should on average be preferred over vanilla snapshots.
%These results are in line with the findings of~\cite{fan2017learning}.

\begin{table}[t]
\caption{Results for the \modelname~model using only content features (baseline), vanilla snapshots, highlighted snapshots, and saliency heatmaps.
All results significantly improve over the \modelname~baseline.
Best results are shown in bold.}
\label{tab:letorvisresults}
\centering
\begin{tabular}{l\OK l\OK l\OK l\OK l\OK l}
\toprule
                      & p@1    & p@10  & ndcg@1  & ndcg@10 & MAP   \\ 
\midrule
\modelname~baseline & 0.338  & 0.370 & 0.189   & 0.233   & 0.415 \\ 
\midrule
VGG snapshots      & 0.514 & 0.484 & 0.292 & 0.324 & 0.442 \\ 
ResNet snapshots   & 0.550 & 0.452 & 0.310 & 0.301 & 0.437 \\ 
VGG highlights     & \textbf{0.560} & \textbf{0.520} & 0.323 & \textbf{0.346} & \textbf{0.456} \\ 
ResNet highlights  & 0.530 & 0.463 & 0.305 & 0.312 & 0.440 \\
\midrule
VGG saliency       & 0.554 & 0.453 & 0.310   & 0.302   & 0.422 \\ 
ResNet saliency    & \textbf{0.560} & 0.476 & \textbf{0.333} & 0.321 & 0.442 \\
\bottomrule
\end{tabular}
\vspace*{-.5\baselineskip}
\end{table}


\subsection{\modelname{} model with saliency heat maps}
The last two rows of Table~\ref{tab:letorvisresults} show the performance of the \modelname{} mo\-del when using synthetic saliency heat maps as an input.
The visual features are learned using both VGG-16 and ResNet-152.
In this case, ResNet-152 consistently outperforms VGG-16.
Although the highlighted snapshots with VGG-16 still outperform ResNet-152 with saliency heat maps on p@10, ndcg@10 and MAP, the saliency heat maps with ResNet-152 match and outperform VGG-16 with highlighted snapshots when looking at p@1 and ndcg@1. 
Hence, saliency heat maps should be preferred in applications where early precision is important,
while highlighted snapshots should be used when a high overall performance is needed. 
%However, it is yet unclear what causes these differences,


\subsection{Baseline comparison}
Table~\ref{tab:baseresults} compares the performance of the \modelname~model to BM25, non-visual \ac{LTR} methods and the ViP model by~\citet{fan2017learning}. 
Specifically, the table shows the performance of VGG-16 with highlighted snapshots and of ResNet-152 with synthetic saliency heatmaps, as these are the best-performing variants of the \modelname~model according to Table~\ref{tab:letorvisresults}.
%Because these two results yield the highest \ac{LTR} performance, they are used to compare the \modelname~model with the baselines. 
Both methods have a significant performance increase compared to BM25, almost doubling the metrics values in many cases.

When comparing to non-visual \ac{LTR} methods, both \modelname~implementations show consistently better performance.
However, not all metrics are improved significantly.
%However, we do find a significant improvement by the \modelname~implementations on the AdaRank \ac{LTR} results.
%Furthermore, when using VGG-16 with highlighted snapshots there is a significant performance increase in ndcg@10 compared to LambdaMart and in p@10 for both RankBoost and LambdaMart.
We attribute this to the fact that, similarly to~\cite{fan2017learning}, the \ac{LTR} component of the \modelname{} model is based on pairwise hinge loss, which is a relatively simple loss function.
%As a future work, we plan to investigate the effect of different loss functions on the performance of the \modelname{} model.

Finally, we compare the \modelname~implementations to ViP, the only existing \ac{LTR} method with visual features.
Here, we clearly see that both our implementations significantly outperform ViP on all metrics.
Also note, that ViP loses to two out of three non-visual \ac{LTR} baselines, namely RankBoost and LambdaMart.
We believe this is due to the reason discussed above: ViP uses pairwise hinge loss as the \ac{LTR} component~\cite{fan2017learning}, which may be suboptimal.

The above results show that the proposed \modelname{} model outperforms baselines, whether they are supervised or unsupervised, use visual features or not.
However, to achieve consistent significant improvements compared to the state-of-the-art \ac{LTR} methods, different loss functions within the \modelname{} model have to be investigated.


\begin{table}[t]
\caption{Results for the VGG-16 with highlighted snapshots, ResNet-152 with saliency heatmaps, and baselines.
$\dagger$ indicates a significant decrease in performance compared to VGG highlights and $\ddagger$ indicates a significant decrease in performance compared to both \modelname{} implementations.
Best results are shown in bold.}

\label{tab:baseresults}
\begin{tabular}{l\OK l\OK l\OK l\OK l\OK l}
\toprule
                      & p@1    & p@10  & ndcg@1  & ndcg@10 & MAP   \\
\midrule
BM25                  & 0.300$^\ddagger$  & 0.316$^\ddagger$ & 0.153$^\ddagger$   & 0.188$^\ddagger$   & 0.350$^\ddagger$ \\ 
\midrule
RankBoost             & 0.450  & 0.444 & 0.258   & 0.288$^\dagger$    & 0.427 \\
AdaRank               & 0.290$^\ddagger$   & 0.357$^\ddagger$  & 0.149$^\ddagger$    & 0.227$^\ddagger$    & 0.398 \\
LambdaMart            & 0.470  & 0.420$^\dagger$ & 0.256   & 0.275$^\dagger$    & 0.418 \\ 
\midrule
ViP snapshots         & 0.392$^\ddagger$ & 0.398$^\ddagger$ & 0.217$^\ddagger$   & 0.254$^\ddagger$   & 0.421$^\ddagger$ \\ 
ViP highlights        & 0.418$^\ddagger$  & 0.416$^\ddagger$ & 0.239$^\ddagger$   & 0.269$^\ddagger$   & 0.422$^\ddagger$ \\
\midrule
VGG highlights        & \textbf{0.560}  & \textbf{0.520} & 0.323   & \textbf{0.346}   & \textbf{0.456} \\ 
ResNet saliency       & \textbf{0.560} & 0.476 & \textbf{0.333} & 0.321 & 0.442 \\
\bottomrule
\end{tabular}
\end{table}
