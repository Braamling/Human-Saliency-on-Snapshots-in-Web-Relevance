% !TEX root = www2019-visual-ltr.tex

\section{Discussion}
\label{sec:discussion}
In this section, we address two practical aspects of the proposed \modelname~model and dataset:
\begin{inparaenum}[(i)]
    \item the number of parameters of the \modelname{} model and corresponding optimizations, and
    \item the performance of content features included in the \datasetname~dataset.
\end{inparaenum}


\subsection{Training and inference optimization} \label{sec:sectionoptimalization}
Both training and inference using deep convolutional networks are generally computationally expensive.
By separating the feature extraction and transformation layers in the proposed \modelname~architecture (see Figure~\ref{fig:multimodelarchitecture}) we allow powerful computational optimizations for both training and inference. 

\if0
In this section we discuss how we 
\begin{inparaenum}[(i)]
    \item optimize training performance by drastically reducing the number of parameters used for forward propagation during training, 
    \item reduce memory requirements during training by storing the output of the frozen feature extraction layers prior to training, and
    \item use query-independent snapshots and synthetic saliency heatmaps to minimize increase in performance requirements when ranking webpages with visual features.
    %\item the styling information is not available together with web pages \todo{what do we mean with this?}.
\end{inparaenum}
\fi

\paragraph{Training optimization} 
Although we freeze the parameters in the visual feature extraction layer (component (2) of Figure~\ref{fig:multimodelarchitecture}) when using a pre-trained model, we still need to compute the output from all the frozen parameters during the forward propagation.
We can avoid the computational cost associated with the forward propagation on the frozen layers by storing the output of the visual feature extraction layer to disk prior to training.
By storing these vectors to disk, we leave only the parameters of the fully connected layers (component (3) of Figure~\ref{fig:multimodelarchitecture}) to be calculated and stored in memory during the forward propagation. 
% \todo{Is this discussion connected to the two paragraphs below? If yes, then how are they connected? If no, then what exactly do we want to say here talking about storing parameters to disk?}

By applying the above procedure, we can reduce the number of parameters of the \modelname~model by the following quantities.
When using VGG-16, the visual feature extraction layer consists of $14,714,688$ parameters, which is $12.3\%$ of the parameters in the \modelname{} model.
When using ResNet-152, the visual feature extraction layer consists of $58,144,239$ parameters, which is $49.5\%$ of the parameters in the \modelname{} model.

By using the stored output of the visual feature extraction layer $x_{vf}$ instead of an actual image $x_{i}$ as the input of the \modelname~model,
we reduce the size of each input by $84.7\%$ and $98.6\%$ for VGG-16 and ResNet-152 respectively.
This reduction in input size further reduces the memory required for training the model.


\paragraph{Real-time inference optimization}
When using \ac{LTR} in a large-scale production environment, the impact of newly introduced \ac{LTR} features (visual features, in our case) on real-time computational requirements is of major concern.
%Here, we discuss the impact of the newly introduced visual features on the real-time inference.
%In order to deliver real-time search results, the model needs to be both powerful and lightweight during inference. 

Since both the vanilla snapshots and synthetic saliency heat\-maps are query independent, the output of the visual feature transformation layer (component (3) of Figure~\ref{fig:multimodelarchitecture}) does not change for different document/query pairs. This allows us to do inference offline, leaving only the scoring component to be inferred in real-time.
Since a visual feature vector is of size $1\times30$, using the offline inferred feature vector would result in a \ac{LTR} model with $30 + 11 = 41$ features. This increase in parameters is merely negligible in terms of computational costs.


\subsection{Benchmarking content features} \label{sec:contentfeatures}
In \ac{LTR} research, both the amount and type of considered features vary widely per dataset and study.
The most recent LETOR 4.0 dataset~\cite{Qin2013:Introducing} contains 46 features extracted for webpages from the GOV2 dataset and queries from the million query tracks 2007 and 2008 (MQ2007 and MQ2008).
In the \datasetname~dataset, we use 11 features that are a subset of the 46 features of LETOR (see Table~\ref{tab:setdescription}).
These 11 features are chosen to be both informative and easy to compute.
Here, we compare our subset of 11 features to the full set of 46 features.
The experiments are run on the GOV2 dataset and MQ2007 queries.
The \ac{LTR} methods considered are the same as in Section~\ref{sec:results}, namely RankBoost, AdaRank, and LambdaMart.

The results of running the considered \ac{LTR} methods using both 46 and 11 features are shown in Table~\ref{tab:11vs46}.
From these results, we see that the number of features has a significant effect on the performance of AdaRank.
However, for the best-performing RankBoost and LambdaRank methods the drop in performance is minor when using 11 features instead of 46 features.
This indicates that the chosen 11 features included in the \datasetname~dataset form a reasonable trade-off between effectiveness and computation cost.

\begin{table}[h]
\caption{Comparison of 46 LETOR features and 11 LETOR features that are also used in \datasetname.}
\label{tab:11vs46}
\centering
\begin{tabular}{lccccc}
\toprule
%& \multicolumn{7}{c}{MQ2007 46 features vs 11 features}                                     \\
           & p@1  & p@10   & ndcg@1 & ndcg@10 & MAP \\ 
\midrule
RankBoost - 46 & 0.453 & 0.371 & 0.391 & 0.430  & 0.457 \\
RankBoost - 11 & 0.448 & 0.372 & 0.381  & 0.431   & 0.453 \\
\midrule
AdaRank - 46  & 0.420 & 0.360 & 0.367 & 0.424  & 0.449 \\
AdaRank - 11  & 0.385 & 0.287 & 0.364  & 0.394   & 0.386 \\ 
\midrule
LambdaMart - 46 & 0.452 & 0.384 & 0.405 & 0.444  & 0.463 \\
LambdaMart - 11 & 0.448 & 0.380 & 0.397  & 0.443   & 0.455 \\
\bottomrule
\end{tabular}
\end{table}



\if0
\subsection{Reproducing ViP on \datasetname{} dataset}
ViP is the only existing \ac{LTR} method that uses visual features~\cite{fan2017learning}.
It was evaluated on the GOV2 dataset, which largest limitation is that webpages in this dataset do not contain images and style information and are, thus, visually homogeneous.
We introduce the \datasetname{} dataset, which is visually diverse and is most suitable for studying \ac{LTR} with visual features.
Here, we reproduce the ViP results on the proposed \datasetname{} dataset.

The results in Table~\ref{tab:baseresults} show the same trend as described by~\citet{fan2017learning}:
the ViP model with vanilla snapshots outperforms the \ac{LTR} baseline without visual features (first row of Table~\ref{tab:letorvisresults}),
while the highlighted snapshots outperform the vanilla ones.

that the limitations in the ViP model become apparent when being used with the more diverse and rich \datasetname~dataset. We do see that the results show a similar pattern as described by ~\citet{fan2017learning} where the model performs better when using highlighted snapshots compared to vanilla snapshots. However, using ViP with vanilla and highlighted snapshots from the \datasetname~dataset is outperformed by both RankBoost and LambdaMart. 
\fi


% VGG16 -               134,383,885
% VGG16 cached:    119,669,197
% difference        14,714,688

% ResNet-152         58,205,709
% ResNet-cached              61,470
% ResNet-modified     59,243,247
% difference         58,144,239

% scores network with images: 750
% scorer network no images: 450