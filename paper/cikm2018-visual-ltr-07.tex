% !TEX root = www2019-visual-ltr.tex

\section{Discussion}
\label{sec:discussion}
In this section, we address two practical aspects of the proposed \modelname~model and dataset:
\begin{inparaenum}[(i)]
    \item the number of parameters of the \modelname~model and corresponding optimizations, and
    \item the performance of content features included in the \datasetname~dataset.
\end{inparaenum}


\subsection{Training and Inference Optimization} \label{sec:sectionoptimalization}
Both training and inference using deep convolutional networks are generally computationally expensive.
The proposed \modelname~architecture (see Figure~\ref{fig:multimodelarchitecture}) introduces two properties that can be used to create powerful optimizations for training and inference.
\todo{What are these two properties?}
In this section, \todo{the structure, announced here, should correspond to the structure of the subsection, i.e., here we should say something about training optimization and inference optimization.}
\todo{For now, I removed the list below (see the source file).}

\if0
we discuss how we
\begin{inparaenum}[(i)]
    \item optimize training performance by drastically reducing the number of parameters used for forward propagation during training, 
    \item reduce memory requirements during training by storing the output of the frozen feature extraction layers prior to training, and
    \item use query-independent snapshots and synthetic saliency heatmaps to minimize increase in performance requirements when ranking webpages with visual features.
    %\item the styling information is not available together with web pages \todo{what do we mean with this?}.
\end{inparaenum}
\fi

\paragraph{Training optimization} 
Although we are freezing the parameters in the visual feature extraction layer (component (2) of Figure~\ref{fig:multimodelarchitecture}) when using a pre-trained model, we still need to compute the output from all the frozen parameters during the forward propagation.
We can avoid the computational cost associated with the forward propagation on the frozen layers by storing the output of the visual feature extraction layer to disk prior to training.
By storing these vectors to disk, we leave only the parameters of the fully connected layers to be calculated during the forward propagation (component (3) of Figure~\ref{fig:multimodelarchitecture}).
\todo{Is this discussion connected to the two paragraphs below? If yes, then how are they connected? If no, then what exactly do we want to say here talking about storing parameters to disk?}

For VGG-16, we convert $3\times224\times224$ input images to vectors of $1\times25088$, thereby reducing the input size by $84.7\%$.
\todo{You never talk about these dimension before. We should first explain them here or in Section 3.}
\todo{How do we convert $3\times224\times224$ to $1\times25088$? Why $1\times25088$?}
We also reduce the total number of parameters by $12.3\%$ \todo{reduced compared to what?} by removing the visual feature extraction layer consisting of $14,714,688$ parameters.

For ResNet-152, we convert $3\times224\times224$ input images to vectors of $1\times2048$, thereby reducing the input size by $98.6\%$.
\todo{Why $1\times2048$ as opposed to  $1\times25088$ for VGG-16?}
The visual feature extraction layer consists of $58,144,239$ parameters, which reduces the total amount of parameters during inference by $49.5\%$.
\todo{Same here: reduced compared to what?}

The above optimizations reduce the computational requirements when training the \modelname~model with both VGG-16 and ResNet-152.
%for ResNet-152 such that the training time of a single epoch decreased from hours to minutes, using a single CPU on a 2015 MacBook Pro. 

\paragraph{Real-time inference optimization}
When using \ac{LTR} in a large-scale production environment, the impact of newly introduced \ac{LTR} features (visual features, in our case) on real-time computational requirements is of major concern.
%Here, we discuss the impact of the newly introduced visual features on the real-time inference.
%In order to deliver real-time search results, the model needs to be both powerful and lightweight during inference. 

Since both the vanilla snapshots and synthetic saliency heat\-maps are query independent, the output of the visual feature transformation layer (component (3) of Figure~\ref{fig:multimodelarchitecture}) is consistent for each document.
\todo{What do you mean by consistency?}
This allows us to do the inference on the visual feature transformation layer separately from the rest of the model inference.
\todo{I am not sure I understand, why this is the case. Can we do this inference offline instead of query-time? If yes, we should say so.}
Doing the inference on the visual feature transformation layer and storing visual features leaves only the scoring component to be inferred in real-time.
Since a visual feature vector is of size $1\times30$, this would increase the number of parameters in the scoring component from $450$ parameters without the visual features to $750$ parameters with visual features.
\todo{Where do we get 450 parameters? Why do we multiply 30 visual features by 10?}
The increase of $300$ parameters is merely negligible compared to the number of parameters in modern deep learning systems.
\todo{But we are talking about real-time. Do we have DL systems with many parameters running in real-time?}


\subsection{Benchmarking Content Features} \label{sec:contentfeatures}
In \ac{LTR} research, both the amount and type of considered features vary widely per dataset and study.
The most recent LETOR 4.0 dataset~\cite{Qin2013:Introducing} contains 46 features extracted for webpages from the GOV2 dataset and queries from the million query tracks 2007 and 2008 (MQ2007 and MQ2008).
In the \datasetname~dataset, we use 11 features that are a subset of the 46 features of LETOR (see Table~\ref{tab:setdescription}).
These 11 features are chosen to be both informative and easy to compute.
Here, we compare our subset of 11 features to the full set of 46 features.
The experiments are run on the GOV2 dataset and MQ2007 queries.
The \ac{LTR} methods considered are the same as in Section~\ref{sec:results}, namely RankBoost, AdaRank, and LambdaMart.

The results of running the considered \ac{LTR} methods using both 46 and 11 features are shown in Table~\ref{tab:11vs46}.
From these results, we see that the number of features has a significant effect on the performance of AdaRank.
However, for the best-performing RankBoost and LambdaRank methods the drop in performance is minor when using 11 features instead of 46 features.
This indicates that the chosen 11 features included in the \datasetname~dataset form a reasonable trade-off between effectiveness and computation cost.

\begin{table}[h]
\caption{Comparison of 46 LETOR features and 11 LETOR features that are also used in \datasetname.}
\label{tab:11vs46}
\centering
\begin{tabular}{lccccc}
\toprule
%& \multicolumn{7}{c}{MQ2007 46 features vs 11 features}                                     \\
           & p@1  & p@10   & ndcg@1 & ndcg@10 & MAP \\ 
\midrule
RankBoost - 46 & 0.453 & 0.371 & 0.391 & 0.430  & 0.457 \\
RankBoost - 11 & 0.448 & 0.372 & 0.381  & 0.431   & 0.453 \\
\midrule
AdaRank - 46  & 0.420 & 0.360 & 0.367 & 0.424  & 0.449 \\
AdaRank - 11  & 0.385 & 0.287 & 0.364  & 0.394   & 0.386 \\ 
\midrule
LambdaMart - 46 & 0.452 & 0.384 & 0.405 & 0.444  & 0.463 \\
LambdaMart - 11 & 0.448 & 0.380 & 0.397  & 0.443   & 0.455 \\
\bottomrule
\end{tabular}
\end{table}



\if0
\subsection{Reproducing ViP on \datasetname{} dataset}
ViP is the only existing \ac{LTR} method that uses visual features~\cite{fan2017learning}.
It was evaluated on the GOV2 dataset, which largest limitation is that webpages in this dataset do not contain images and style information and are, thus, visually homogeneous.
We introduce the \datasetname{} dataset, which is visually diverse and is most suitable for studying \ac{LTR} with visual features.
Here, we reproduce the ViP results on the proposed \datasetname{} dataset.

The results in Table~\ref{tab:baseresults} show the same trend as described by~\citet{fan2017learning}:
the ViP model with vanilla snapshots outperforms the \ac{LTR} baseline without visual features (first row of Table~\ref{tab:letorvisresults}),
while the highlighted snapshots outperform the vanilla ones.

that the limitations in the ViP model become apparent when being used with the more diverse and rich \datasetname~dataset. We do see that the results show a similar pattern as described by ~\citet{fan2017learning} where the model performs better when using highlighted snapshots compared to vanilla snapshots. However, using ViP with vanilla and highlighted snapshots from the \datasetname~dataset is outperformed by both RankBoost and LambdaMart. 
\fi


% VGG16 -               134,383,885
% VGG16 cached:    119,669,197
% difference        14,714,688

% ResNet-152         58,205,709
% ResNet-cached              61,470
% ResNet-modified     59,243,247
% difference         58,144,239

% scores network with images: 750
% scorer network no images: 450