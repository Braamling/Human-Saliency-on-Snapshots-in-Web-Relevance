% !TEX root = www2019-visual-ltr.tex

\section{Introduction}
The design and appearance of a webpage are determining factors for a user to examine the page or to divert to another page~\cite{nielsen1999designing,nielsen2006f,pernice2017f,wang2014eye}.
However, relatively little is known about the potential of visual appearance to help determine the perceived relevance of a webpage.
Recently, visual features, extracted from snapshots of webpages and \acp{SERP}, have been introduced into \acf{LTR}
and have been shown to significantly improve the \ac{LTR} performance~\cite{fan2017learning,zhang2018relevance}.
%in addition to content features, such as BM25, quality indicators, such as PageRank, and behavioral features, such as CTR.
%These are the first steps towards \ac{LTR} with visual features and there is currently a large room for improvement.

In this paper we continue studying \ac{LTR} with visual features and propose the \modelnamef~model
that integrates state-of-the-art visual features extraction methods.
We present two implementations of the \modelname~model.
First, we extract visual features from webpage snapshots using transfer learning and, in particular, by adopting the VGG-16~\cite{simonyan2014very} and ResNet-152~\cite{he2016deep} models pre-trained on ImageNet.
Second, we introduce a novel set of visual features extracted from synthetic saliency heatmaps, which explicitly model how users view webpages~\cite{shan2017two}.

Currently, there is no dataset available to support research on \ac{LTR} with visual features.
\citet{fan2017learning} experimented with webpages from the GOV2 collection.\footnote{\url{http://ir.dcs.gla.ac.uk/test_collections/gov2-summary.htm}}
However, GOV2 solely contains webpages within the .gov domain, i.e., pages with a relatively narrow scope,
and, more importantly, these webpages do not contain their original images and styles.
Due to this, the GOV2 collection cannot fully support research on \ac{LTR} with visual features, as it does not reflect visually rich and diverse webpages found on the internet today.

To overcome this issue, we release the \datasetname~dataset that contains webpages from the ClueWeb12 collection\footnote{\url{https://lemurproject.org/clueweb12/}}
and queries from the TREC Web Tracks 2013 \& 2014~\cite{collins2013trec,collins2015trec}.
For each webpage of ClueWeb12 that also appears in the web tracks, the \datasetname~dataset contains a snapshot
and a set of content features, such as BM25 and PageRank.
The introduced dataset is made publicly available.\footnote{\url{https://github.com/Braamling/learning-to-rank-webpages-based-on-visual-features/blob/master/dataset.md}}

To assess the performance of the proposed \modelname~model, we run a set of experiments on the introduced \datasetname~dataset.
First, our experiments confirm that visual features significantly improve the \ac{LTR} performance, which is inline with previous findings~\cite{fan2017learning}.
Second, we show that both implementations of the \modelname~model, namely transfer learning (with VGG-16 and ResNet-152) and synthetic saliency heatmaps,
significantly outperform other \ac{LTR} methods with visual features.

\if0
The work by \citet{fan2017learning} indicates that the visual appearance of a webpage can have a significant impact on perceived utility, which opens a new direction in web search and \ac{LTR}.
However, there are several limitations in \cite{fan2017learning}.
First, the method used to extract visual features in the ViP architecture
\begin{inparaenum}[(i)]
\item is based on a strong assumption that users view websites in an F-shape pattern, 
\item grayscales and shrinks the input images to $64\times64$, and
\item does not use a state-of-the-art visual feature extraction method.
\end{inparaenum}

A second limitation in~\citep{fan2017learning} is that
the rendered webpages come from the GOV2 collection.\footnote{\url{http://ir.dcs.gla.ac.uk/test_collections/gov2-summary.htm}}
This collection is limited in the sense that 
\begin{inparaenum}[(i)]
\item it solely contains webpages within the .gov domain crawled in 2004, i.e., somewhat outdated pages with a relatively narrow scope, and
\item the webpages in the dataset do not contain their original images.
%\item the styling information is not available together with webpages \todo{what do we mean with this?}.
\end{inparaenum}
This dataset is not suitable for research on \ac{LTR} with visual features because it does not reflect the diverse and rich webpages (e.g., styling, images) found on the internet today. 
Additionally, the webpage snapshots have not been made publically available, making it difficult to reproduce results. 
In order to advance research on \ac{LTR} with visual features, it is important to have a public dataset with more diverse and up-to-date documents and richer visual information.
\fi


In summary, the main contributions of this work are:
\begin{inparaenum}[(i)]
\item We introduce transfer learning to extract visual features for \ac{LTR}.
\item We introduce synthetic saliency heatmaps as a novel set of \ac{LTR} features.
\item We introduce and publish \datasetname, an out-of-the-box dataset for \ac{LTR} with visual features.
\end{inparaenum}


\if0
\todo{Revise this if we have space or drop it.} The rest of the paper is organized as follows. Section~\ref{sec:dataset} describes the collection process that was used to construct \datasetname. In Section~\ref{sec:experiments} we reproduce the work of \citet{fan2017learning} and demonstrate various feature extraction methods on \datasetname~and set a baseline for future visual \ac{LTR} research.  
\fi