% !TEX root = www2019-visual-ltr.tex

\section{Introduction}
The design and appearance of a webpage are determining factors for a user to examine the page or divert to another page~\cite{nielsen1999designing,nielsen2006f,pernice2017f,wang2014eye}.
However, relatively little is known about the potential of visual features to help determine the perceived relevance of a (web) page. In \acf{LTR}, visual features can be used in addition to content features such as BM25, quality indicators such as PageRank, and behavioral features such as CTR.
Recently, \citet{fan2017learning} have introduced ViP, a \ac{LTR} model that uses a combination of both visual and content features.
Visual features are calculated from snapshots, which are in turn created by rendering webpages.
The authors demonstrate that adding these visual features helps to significantly improve the \ac{LTR} performance.

The work by \citet{fan2017learning} indicates that the visual appearance of a webpage can have a significant impact on perceived utility, which opens a new direction in web search and \ac{LTR}.
However, there are several limitations in \cite{fan2017learning}.
First, the method used to extract visual features in the ViP architecture
\begin{inparaenum}[(i)]
\item is based on a strong assumption that users view websites in an F-shape pattern, 
\item grayscales and shrinks the input images to $64\times64$, and
\item does not use a state-of-the-art visual feature extraction method.
\end{inparaenum}

A second limitation in~\citep{fan2017learning} is that
the rendered webpages come from the GOV2 collection.\footnote{\url{http://ir.dcs.gla.ac.uk/test_collections/gov2-summary.htm}}
This collection is limited in the sense that 
\begin{inparaenum}[(i)]
\item it solely contains webpages within the .gov domain crawled in 2004, i.e., somewhat outdated pages with a relatively narrow scope, and
\item the webpages in the dataset do not contain their original images.
%\item the styling information is not available together with webpages \todo{what do we mean with this?}.
\end{inparaenum}
In order to advance research on \ac{LTR} with visual features, it is important to have a dataset with more diverse and up-to-date documents and richer visual information (e.g., styling, images).

In this work, we address the above limitations.
First, we propose a multimodal architecture for comparing the performance of using modern feature extraction methods on webpage snapshots. Using transfer learning, we transform a pretrained VGG-16 and ResNet-152 ~\cite{simonyan2014very, he2016deep} model into a powerful \ac{LTR} visual feature extractor. These transfer learned models do not suffer from the previously mentioned limitations from \cite{fan2017learning} and show a significant increase in retrieval performance.

Second, we introduce a novel \ac{LTR} feature by training a model to explicitly model user web viewing patterns by using synthetic saliency heatmaps. Other than increasing the retrieval performance compared to traditional \ac{LTR} methods, these saliency heatmaps also have a significantly lower storage requirement than using webpage snapshots.

Third, we release \datasetname, a dataset that contains rich and highly diverse snapshots from the ClueWeb12 collection.\footnote{\url{https://lemurproject.org/clueweb12/}} The snapshots are acquired for judged documents in the TREC Web Track 2013 \& 2014~\cite{collins2013trec,collins2015trec}. For each document, we also calculate content features, such as BM25 and TF-IDF, that can be used for \ac{LTR}.
The proposed dataset is made publicly available.\footnote{URL removed for review.}
%\todo{describe key features of the proposed dataset by unfolding the next sentence into a few sentences}
%
%We believe that this dataset allows the \ac{LTR} research community to investigate various methods of using visual features for search and ranking.


In summary, the main contributions of this work are:
\begin{inparaenum}[(i)]
\item We introduce a multimodal architecture to improve \ac{LTR} with visual features by applying powerful transfer learning visual feature vectors.
\item We introduce synthetic saliency heatmaps as a novel \ac{LTR} feature. 
\item We propose and publish \datasetname, an out-of-the-box dataset for \ac{LTR} with visual features.
\end{inparaenum}

\if0
\todo{Revise this if we have space or drop it.} The rest of the paper is organized as follows. Section~\ref{sec:dataset} describes the collection process that was used to construct \datasetname. In Section~\ref{sec:experiments} we reproduce the work of \citet{fan2017learning} and demonstrate various feature extraction methods on \datasetname~and set a baseline for future visual \ac{LTR} research.  
\fi