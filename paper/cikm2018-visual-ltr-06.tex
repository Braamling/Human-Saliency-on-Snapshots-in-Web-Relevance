% !TEX root = cikm2018-visual-ltr.tex

\begin{table}[h]
\caption{Comparison between different \ac{LTR} methods with visual features. $\dagger$ indicates a significant improvement on ViP baseline and $\ddagger$ indicates an improvement on ViP highlights.}
\label{tab:visresults}
\centering
\begin{tabular}{llllll}
\toprule
                      & p@1    & p@10  & ndcg@1  & ndcg@10 & MAP   \\ 
\midrule
ViP baseline          & 0.338  & 0.370 & 0.189   & 0.233   & 0.415 \\ 
\midrule
ViP snapshots         & 0.392$^\dagger$ & 0.398$^\dagger$ & 0.217   & 0.254$^\dagger$   & 0.421 \\ 
ViP highlights        & 0.418$^\dagger$  & 0.416$^\dagger$ & 0.239$^\dagger$   & 0.269$^\dagger$   & 0.422 \\
\midrule
VGG snapshots      & 0.514$^\ddagger$    & 0.484$^\ddagger$ & 0.292$^\ddagger$   & 0.324$^\ddagger$   & 0.442$^\ddagger$ \\ 
VGG highlights     & 0.560$^\ddagger$    & 0.520$^\ddagger$ & 0.323$^\ddagger$   & 0.346$^\ddagger$   & 0.456$^\ddagger$ \\ 
\midrule
VGG saliency       & 0.554$^\ddagger$    & 0.453$^\ddagger$ & 0.310$^\ddagger$   & 0.302$^\ddagger$   & 0.422 \\ 
\bottomrule
\end{tabular}
\end{table}

\begin{table}[h]
\caption{Comparison of the VGG-16 model using highlights to content-based baselines. $*$ indicates a significant drop in performance compared to VGG highlights. }

\label{tab:baseresults}
\begin{tabular}{llllll}
\toprule
                      & p@1    & p@10  & ndcg@1  & ndcg@10 & MAP   \\
\midrule
BM25                  & 0.300$^*$  & 0.316$^*$ & 0.153$^*$   & 0.188$^*$   & 0.350$^*$ \\ 
\midrule
RankBoost             & 0.450  & 0.444 & 0.258   & 0.288$^*$    & 0.427 \\
AdaRank               & 0.290$^*$   & 0.357$^*$  & 0.149$^*$    & 0.227$^*$    & 0.398 \\
LambdaMart            & 0.470  & 0.420$^*$ & 0.256   & 0.275$^*$    & 0.418 \\ 
\midrule
VGG highlights        & 0.560  & 0.520 & 0.323   & 0.346   & 0.456 \\ 
\bottomrule
\end{tabular}
\end{table}


\section{Experiments and Results}
In this section, we discuss experiments performed on the proposed \datasetname~dataset.
These experiments are set out to demonstrate the abilities of visual features in LTR and set a baseline for future visual LTR research.

\if0
Table \ref{tab:11vs46} shows the results of the benchmark experiments. Each model was trained by optimizing the $P@5$ on the validation set. The results show that using the features from \datasetname~compared to the feature from LETOR has a minor impact on \ac{LTR} performance.
\fi

\paragraph{Experimental setup}
\todo{Say about the ViP model and how we use it (I guess, exactly as in their paper). In particular, explain the difference between ViP baseline, ViP snapshots and ViP highlights.}
\todo{Say what methods we compare to it. Explain VGG snapshots, VGG highlights, VGG saliency.}
\todo{Say about non-visual baselines.}

Each experiment is run five times for each of the five folds. \todo{Why? Due to random initialization?}
Each run is performed using the Adam optimizer with a batch size of 100 using a learning rate of $0.0001$ and $0.00005$ for the ViP and VGG-16 models respectively. \todo{Why these values?}
Within each fold, the results for all five runs are averaged to create one measurement per fold per query.

We use the following retrieval performance measures: precision and NDCG at $\{1,10\}$ and MAP.
Significance is determined using a two tailed paired t-test (p-value $\leq 0.05$). 

\paragraph{Reproducing ViP}
Table~\ref{tab:visresults} shows the results using the different visual ranking methods and the ViP baseline. 
Results marked with a $\dagger$ have a significance improvement compared to the ViP baseline model. 
We see that in terms of significance the results are similar to the work of \citet{fan2017learning}, where the snapshots with highlights yield better performance than using the vanilla snapshots.
\todo{Can we say anything else here? (This is not necessary, just asking)}

\paragraph{Improved feature extraction methods}
The $\ddagger$ sign in Table~\ref{tab:visresults} indicates results where the VGG-16 model significantly outperforms the ViP model with highlighted snapshots. The results clearly show that introducing more powerful visual feature extraction methods significantly increases the \ac{LTR} performance.
\todo{Can we say anything else here? (This is not necessary, just asking)}

\paragraph{Content-based baselines}
Table \ref{tab:baseresults} compares the results of the VGG-16 model using highlighted snapshots with BM25, RankBoost, AdaRank and Lambdamart. The baselines with a significant difference from the VGG-16 model have been marked with a $*$. Although VGG-16 produces higher average results than the baseline methods, it is not possible to proof the statistical significance for some RankBoost and LambdaMart results. These results suggest that combining the visual features using more sophisticated ranking method could further improve \ac{LTR} performance using \datasetname. 

\todo{Here we can add the benchmarking experiment that I left out for now.}