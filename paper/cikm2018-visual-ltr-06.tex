% !TEX root = cikm2018-visual-ltr.tex

\section{Discussion}
\label{sec:discussion}
\subsection{Benchmarking Content Features}

% \todo{In fact, this should go into the discussion and not experimental setup and/or experiments. But let's do this later.}
In \ac{LTR} research, both the amount and type of non-visual features used for training vary widely per dataset and study.
To assess the performance of the 11 non-visual features used in this study, we use a set of known LTR algorithms on the million query track 2007 (MQ2007)\cite{allan2007million} with the LETOR 4.0\footnote{\url{https://www.microsoft.com/en-us/research/project/letor-learning-rank-information-retrieval/}} non-visual features.
MQ2007 and LETOR 4.0 have a total of 46 features containing all 11 features from Table~\ref{tab:setdescription}.
The experiment uses the RankLib implementations of RankBoost, AdaRank, and LambdaMart on both 46 and 11 features using their default settings. 

The results of all three ranking methods using both 46 and 11 features are shown in Table~\ref{tab:11vs46}.
From the results we see that the number of non-visual features has a significant effect on the performance of AdaRank. However, the RankBoost and LambdaRank comparison suggests that the features used in the \datasetname~dataset are powerful enough to be used for \ac{LTR} research.
%TODO Add a tiny conclusion on these results.

\begin{table}[h]
\caption{Benchmark comparison on MQ2007 using all 46 LETOR features and the 11 LETOR features that are also used in \datasetname.}
\label{tab:11vs46}
\centering
\begin{tabular}{lccccc}
\toprule
%& \multicolumn{7}{c}{MQ2007 46 features vs 11 features}                                     \\
           & p@1  & p@10   & ndcg@1 & ndcg@10 & MAP \\ 
\midrule
RankBoost - 46 & 0.453 & 0.371 & 0.391 & 0.430  & 0.457 \\
RankBoost - 11 & 0.448 & 0.372 & 0.381  & 0.431   & 0.453 \\
\midrule
AdaRank - 46  & 0.420 & 0.360 & 0.367 & 0.424  & 0.449 \\
AdaRank - 11  & 0.385 & 0.287 & 0.364  & 0.394   & 0.386 \\ 
\midrule
LambdaMart - 46 & 0.452 & 0.384 & 0.405 & 0.444  & 0.463 \\
LambdaMart - 11 & 0.448 & 0.380 & 0.397  & 0.443   & 0.455 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Training and Inference Optimization}
Both training and inference using deep convolutional networks are generally computationally expensive.
The proposed multimodal \modelname~architecture introduces two properties that can be used to create powerful optimizations for training and inference. In this section, we discuss how we: \begin{inparaenum}[(i)]
\item optimize training performance by drastically reducing the number of parameters used for forward propagation during training, 
\item reduce memory requirements during training by storing the output of the frozen feature extraction layers prior to training, and
\item use the query-independent snapshots and synthetic saliency heatmaps to minimize increase in performance requirements when ranking webpages with visual features.
%\item the styling information is not available together with web pages \todo{what do we mean with this?}.
\end{inparaenum}

\paragraph{Training optimization}
Although we are freezing the parameters in the visual feature extraction layer when using a pre-trained model, we still need to compute the output from all the frozen parameters during the forward propagation. We can avoid the computational cost associated with the forward propagation on the frozen layers by storing the output of the visual feature extraction layer to disk prior to training. By storing these vectors to disk, we leave only the parameters of the fully connected layers to be calculated during the forward propagation. 

For VGG-16 we converted all $3\times224\times224$ input images to vectors of $1\times25088$, thereby reducing the input size by $84.7\%$. We also reduce the total number of parameters by $12.3\%$ by removing the visual feature extraction layer consisting of $14,714,688$ parameters.

For ResNet-152 we convert each $3\times224\times224$ input image to vectors of $1\times2048$, thereby reducing the input size by $98.6\%$. The visual feature extraction layer consists of $58,144,239$ parameters, which reduces the total amount of parameters during inference by $49.5\%$.

These optimizations reduced the computational requirements for ResNet-152 such that the training time of a single epoch decreased from hours minutes, using a single CPU on a 2015 MacBook Pro. 

\paragraph{Inference optimization}
When using \ac{LTR} in a large-scale production environment, the impact of new \ac{LTR} features on computational requirements is a major concern. 
In order to deliver real-time search results, the model needs to be both powerful and lightweight during inference. 

Because both the vanilla snapshots and synthetic saliency images are query independent, the output of the visual feature transformation layer will be consistent for each document. This allows us to do the inference on the visual feature transformation separate from the rest of the model inference. Doing the inference on the visual feature transformation layers and storing the visual feature vectors leaves only the scoring component during real-time inference. Because each of the visual feature vectors are only of size $1\times30$, this would increase the number of parameters in the scoring component from $450$ parameters without the visual features to $750$ parameters with visual features. This increase of $300$ parameters is merely negligible compared to the number of parameters in modern deep learning systems. 


% VGG16 -               134,383,885
% VGG16 cached:    119,669,197
% difference        14,714,688

% ResNet-152         58,205,709
% ResNet-cached              61,470
% ResNet-modified     59,243,247
% difference         58,144,239

% scores network with images: 750
% scorer network no images: 450